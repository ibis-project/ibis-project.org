{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Write your analytics code once, run it everywhere. Main features Ibis provides a standard way to write analytics code, that then can be run in multiple engines. Full coverage of SQL features : You can code in Ibis anything you can implement in a SQL SELECT Transparent to SQL implementation differences : Write standard code that translate to any SQL syntax High performance execution : Execute at the speed of your backend, not your local computer Integration with community data formats and tools (e.g. pandas, Parquet, Avro...) Supported engines Standard DBMS: PostgreSQL , MySQL , SQLite Analytical DBMS: OmniSciDB , ClickHouse Distributed platforms: Impala , PySpark , BigQuery In memory execution: pandas , Dask Example The next example is all the code you need to connect to a database with a countries database, and compute the number of citizens per squared kilometer in Asia: >>> import ibis >>> db = ibis . sqlite . connect ( 'geography.db' ) >>> countries = db . table ( 'countries' ) >>> asian_countries = countries . filter ( countries [ 'continent' ] == 'AS' ) >>> density_in_asia = asian_countries [ 'population' ] . sum () / asian_countries [ 'area_km2' ] . sum () >>> density_in_asia . execute () 130.7019141926602 Learn more about Ibis in our tutorial . Comparison to other tools Why not use pandas ? pandas is great for many use cases. But pandas loads the data into the memory of the local host, and performs the computations on it. Ibis instead, leaves the data in its storage, and performs the computations there. This means that even if your data is distributed, or it requires GPU accelarated speed, Ibis code will be able to benefit from your storage capabilities. Why not use SQL? SQL is widely used and very convenient when writing simple queries. But as the complexity of operations grow, SQL can become very difficult to deal with. With Ibis, you can take fully advantage of software engineering techniques to keep your code readable and maintainable, while writing very complex analytics code. Why not use SQLAlchemy ? SQLAlchemy is very convenient as an ORM (Object Relational Mapper), providing a Python interface to SQL databases. Ibis uses SQLAlchemy internally, but aims to provide a friendlier syntax for analytics code. And Ibis is also not limited to SQL databases, but also can connect to distributed platforms and in-memory representations. Why not use Dask ? Dask provides advanced parallelism, and can distribute pandas jobs. Ibis can process data in a similar way, but for a different number of backends. For example, given a Spark cluster, Ibis allows to perform analytics using it, with a familiar Python syntax. Ibis supports Dask as a backend.","title":"Home"},{"location":"#main-features","text":"Ibis provides a standard way to write analytics code, that then can be run in multiple engines. Full coverage of SQL features : You can code in Ibis anything you can implement in a SQL SELECT Transparent to SQL implementation differences : Write standard code that translate to any SQL syntax High performance execution : Execute at the speed of your backend, not your local computer Integration with community data formats and tools (e.g. pandas, Parquet, Avro...)","title":"Main features"},{"location":"#supported-engines","text":"Standard DBMS: PostgreSQL , MySQL , SQLite Analytical DBMS: OmniSciDB , ClickHouse Distributed platforms: Impala , PySpark , BigQuery In memory execution: pandas , Dask","title":"Supported engines"},{"location":"#example","text":"The next example is all the code you need to connect to a database with a countries database, and compute the number of citizens per squared kilometer in Asia: >>> import ibis >>> db = ibis . sqlite . connect ( 'geography.db' ) >>> countries = db . table ( 'countries' ) >>> asian_countries = countries . filter ( countries [ 'continent' ] == 'AS' ) >>> density_in_asia = asian_countries [ 'population' ] . sum () / asian_countries [ 'area_km2' ] . sum () >>> density_in_asia . execute () 130.7019141926602 Learn more about Ibis in our tutorial .","title":"Example"},{"location":"#comparison-to-other-tools","text":"","title":"Comparison to other tools"},{"location":"#why-not-use-pandas","text":"pandas is great for many use cases. But pandas loads the data into the memory of the local host, and performs the computations on it. Ibis instead, leaves the data in its storage, and performs the computations there. This means that even if your data is distributed, or it requires GPU accelarated speed, Ibis code will be able to benefit from your storage capabilities.","title":"Why not use pandas?"},{"location":"#why-not-use-sql","text":"SQL is widely used and very convenient when writing simple queries. But as the complexity of operations grow, SQL can become very difficult to deal with. With Ibis, you can take fully advantage of software engineering techniques to keep your code readable and maintainable, while writing very complex analytics code.","title":"Why not use SQL?"},{"location":"#why-not-use-sqlalchemy","text":"SQLAlchemy is very convenient as an ORM (Object Relational Mapper), providing a Python interface to SQL databases. Ibis uses SQLAlchemy internally, but aims to provide a friendlier syntax for analytics code. And Ibis is also not limited to SQL databases, but also can connect to distributed platforms and in-memory representations.","title":"Why not use SQLAlchemy?"},{"location":"#why-not-use-dask","text":"Dask provides advanced parallelism, and can distribute pandas jobs. Ibis can process data in a similar way, but for a different number of backends. For example, given a Spark cluster, Ibis allows to perform analytics using it, with a familiar Python syntax. Ibis supports Dask as a backend.","title":"Why not use Dask?"},{"location":"getting_started/","text":"Getting started Installation instructions The next steps show the recommended way of installing ibis. Other installation options can be found in the advanced installation page . Download Anaconda for your operating system and the latest Python version, run the installer, and follow the steps. Detailed instructions on how to install Anaconda can be found in the Anaconda documentation ). In a shell prompt install ibis-framework : conda install -c conda-forge ibis-framework In the same shell prompt, import ibis and print its version python -c 'import ibis; print(ibis.__version__)' You're ready to start using ibis! Tutorials Learn more about ibis in the tutorials !","title":"Getting Started"},{"location":"getting_started/#getting-started","text":"","title":"Getting started"},{"location":"getting_started/#installation-instructions","text":"The next steps show the recommended way of installing ibis. Other installation options can be found in the advanced installation page . Download Anaconda for your operating system and the latest Python version, run the installer, and follow the steps. Detailed instructions on how to install Anaconda can be found in the Anaconda documentation ). In a shell prompt install ibis-framework : conda install -c conda-forge ibis-framework In the same shell prompt, import ibis and print its version python -c 'import ibis; print(ibis.__version__)' You're ready to start using ibis!","title":"Installation instructions"},{"location":"getting_started/#tutorials","text":"Learn more about ibis in the tutorials !","title":"Tutorials"},{"location":"release_notes/","text":"Release Notes 2.1.1 (2022-01-12) Bug Fixes setup.py: set the correct version number for 2.1.0 ( f3d267b ) 2.1.0 (2022-01-12) Bug Fixes consider all packages' entry points ( b495cf6 ) datatypes: infer bytes literal as binary #2915 ( #3124 ) ( 887efbd ) deps: bump minimum dask version to 2021.10.0 ( e6b5c09 ) deps: constrain numpy to ensure wheels are used on windows ( 70c308b ) deps: update dependency clickhouse-driver to ^0.1 || ^0.2.0 ( #3061 ) ( a839d54 ) deps: update dependency geoalchemy2 to >=0.6,<0.11 ( 4cede9d ) deps: update dependency pyarrow to v6 ( #3092 ) ( 61e52b5 ) don't force backends to override do_connect until 3.0.0 ( 4b46973 ) execute materialized joins in the pandas and dask backends ( #3086 ) ( 9ed937a ) literal: allow creating ibis literal with uuid ( #3131 ) ( b0f4f44 ) restore the ability to have more than two option levels ( #3151 ) ( fb4a944 ) sqlalchemy: fix correlated subquery compilation ( 43b9010 ) sqlite: defer db connection until needed ( #3127 ) ( 5467afa ), closes #64 Features allow column_of to take a column expression ( dbc34bb ) ci: More readable workflow job titles ( #3111 ) ( d8fd7d9 ) datafusion: initial implementation for Arrow Datafusion backend ( 3a67840 ), closes #2627 datafusion: initial implementation for Arrow Datafusion backend ( 75876d9 ), closes #2627 make dayofweek impls conform to pandas semantics ( #3161 ) ( 9297828 ) Reverts \"ci: install gdal for fiona\" ( 8503361 ) 2.0.0 (2021-10-06) Features Serialization-deserialization of Node via pickle is now byte compatible between different processes ( #2938 ) Support joining on different columns in ClickHouse backend ( #2916 ) Support summarization of empty data in Pandas backend ( #2908 ) Unify implementation of fillna and isna in Pyspark backend ( #2882 ) Support binary operation with Timedelta in Pyspark backend ( #2873 ) Add group_concat operation for Clickhouse backend ( #2839 ) Support comparison of ColumnExpr to timestamp literal ( #2808 ) Make op schema a cached property ( #2805 ) Implement .insert() for SQLAlchemy backends ( #2613 , #2613 ) Infer categorical and decimal Series to more specific Ibis types in Pandas backend ( #2792 ) Add startswith and endswith operations ( #2790 ) Allow more flexible return type for UDFs ( #2776 , #2797 ) Implement Clip in the Pyspark backend ( #2779 ) Use ndarray as array representation in Pandas backend ( #2753 ) Support Spark filter with window operation ( #2687 ) Support context adjustment for udfs for pandas backend ( #2646 ) Add auth_local_webserver , auth_external_data , and auth_cache parameters to BigQuery connect method. Set auth_local_webserver to use a local server instead of copy-pasting an authorization code. Set auth_external_data to true to request additional scopes required to query Google Drive and Sheets. Set auth_cache to reauth or none to force reauthentication. ( #2655 ) Add bit_and , bit_or , and bit_xor integer column aggregates (BigQuery and MySQL backends) ( #2641 ) Backends are defined as entry points ( #2379 ) Add ibis.array for creating array expressions ( #2615 ) Implement Not operation in PySpark backend ( #2607 ) Added support for case/when in PySpark backend ( #2610 ) Add support for np.array as literals for backends that already support lists as literals ( #2603 ) Bugs Fix data races in impala connection pool accounting ( #2991 ) Fix null literal compilation in the Clickhouse backend ( #2985 ) Fix order of limit and offset parameters in the Clickhouse backend ( #2984 ) Replace equals operation for geospatial datatype to geo_equals ( #2956 ) Fix .drop(fields). The argument can now be either a list of strings or a string. ( #2829 ) Fix projection on differences and intersections for SQL backends ( #2845 ) Backends are loaded in a lazy way, so third-party backends can import Ibis without circular imports ( #2827 ) Disable aggregation optimization due to N squared performance ( #2830 ) Fix .cast() to array outputting list instead of np.array in Pandas backend ( #2821 ) Fix aggregation with mixed reduction datatypes (array + scalar) on Dask backend ( #2820 ) Fix error when using reduction UDF that returns np.array in a grouped aggregation ( #2770 ) Fix time context trimming error for multi column udfs in pandas backend ( #2712 ) Fix error during compilation of range_window in base_sql backends (:issue: 2608 ) ( #2710 ) Fix wrong row indexing in the result for 'window after filter' for timecontext adjustment ( #2696 ) Fix aggregate exploding the output of Reduction ops that return a list/ndarray ( #2702 ) Fix issues with context adjustment for filter with PySpark backend ( #2693 ) Add temporary struct col in pyspark backend to ensure that UDFs are executed only once ( #2657 ) Fix BigQuery connect bug that ignored project ID parameter ( #2588 ) Fix overwrite logic to account for DestructColumn inside mutate API ( #2636 ) Fix fusion optimization bug that incorrectly changes operation order ( #2635 ) Fixes a NPE issue with substr in PySpark backend ( #2610 ) Fixes binary data type translation into BigQuery bytes data type ( #2354 ) Make StructValue picklable ( #2577 ) Support Improvement of the backend API. The former Client subclasses have been replaced by a Backend class that must subclass ibis.backends.base.BaseBackend . The BaseBackend class contains abstract methods for the minimum subset of methods that backends must implement, and their signatures have been standardized across backends. The Ibis compiler has been refactored, and backends don't need to implement all compiler classes anymore if the default works for them. Only a subclass of ibis.backends.base.sql.compiler.Compiler is now required. Backends now need to register themselves as entry points. ( #2678 ) Deprecate exists_table(table) in favor of table in list_tables() ( #2905 ) Remove handwritten type parser; parsing errors that were previously IbisTypeError are now parsy.ParseError . parsy is now a hard requirement. ( #2977 ) Methods current_database and list_databases raise an exception for backends that do not support databases ( #2962 ) Method set_database has been deprecated, in favor of creating a new connection to a different database ( #2913 ) Removed log method of clients, in favor of verbose_log option ( #2914 ) Output of Client.version returned as a string, instead of a setuptools Version ( #2883 ) Deprecated list_schemas in SQLAlchemy backends in favor of list_databases ( #2862 ) Deprecated ibis.<backend>.verify() in favor of capturing exception in ibis.<backend>.compile() ( #2865 ) Simplification of data fetching. Backends don't need to implement Query anymore ( #2789 ) Move BigQuery backend to a separate repository <https://github.com/ibis-project/ibis-bigquery> _. The backend will be released separately, use pip install ibis-bigquery or conda install ibis-bigquery to install it, and then use as before. ( #2665 ) Supporting SQLAlchemy 1.4, and requiring minimum 1.3 ( #2689 ) Namespace time_col config, fix type check for trim_with_timecontext for pandas window execution ( #2680 ) Remove deprecated ibis.HDFS , ibis.WebHDFS and ibis.hdfs_connect ( #2505 ) 1.4.0 (2020-11-07) Features Add Struct.from_dict ( #2514 ) Add hash and hashbytes support for BigQuery backend ( #2310 ) Support reduction UDF without groupby to return multiple columns for Pandas backend ( #2511 ) Support analytic and reduction UDF to return multiple columns for Pandas backend ( #2487 ) Support elementwise UDF to return multiple columns for Pandas and PySpark backend ( #2473 ) FEAT: Support Ibis interval for window in pyspark backend ( #2409 ) Use Scope class for scope in pyspark backend ( #2402 ) Add PySpark support for ReductionVectorizedUDF ( #2366 ) Add time context in scope in execution for pandas backend ( #2306 ) Add start_point and end_point to PostGIS backend. ( #2081 ) Add set difference to general ibis api ( #2347 ) Add rowid expression, supported by SQLite and OmniSciDB ( #2251 ) Add intersection to general ibis api ( #2230 ) Add application_name argument to ibis.bigquery.connect to allow attributing Google API requests to projects that use Ibis. ( #2303 ) Add support for casting category dtype in pandas backend ( #2285 ) Add support for Union in the PySpark backend ( #2270 ) Add support for implementign custom window object for pandas backend ( #2260 ) Implement two level dispatcher for execute_node ( #2246 ) Add ibis.pandas.trace module to log time and call stack information. ( #2233 ) Validate that the output type of a UDF is a single element ( #2198 ) ZeroIfNull and NullIfZero implementation for OmniSciDB ( #2186 ) IsNan implementation for OmniSciDB ( #2093 ) [OmnisciDB] Support add_columns and drop_columns for OmnisciDB table ( #2094 ) Create ExtractQuarter operation and add its support to Clickhouse, CSV, Impala, MySQL, OmniSciDB, Pandas, Parquet, PostgreSQL, PySpark, SQLite and Spark ( #2175 ) Add translation rules for isnull() and notnull() for pyspark backend ( #2126 ) Add window operations support to SQLite ( #2232 ) Implement read_csv for omniscidb backend ( #2062 ) [OmniSciDB] Add support to week extraction ( #2171 ) Date, DateDiff and TimestampDiff implementations for OmniSciDB ( #2097 ) Create ExtractWeekOfYear operation and add its support to Clickhouse, CSV, MySQL, Pandas, Parquet, PostgreSQL, PySpark and Spark ( #2177 ) Add initial support for ibis.random function ( #2060 ) Added epoch_seconds extraction operation to Clickhouse, CSV, Impala, MySQL, OmniSciDB, Pandas, Parquet, PostgreSQL, PySpark, SQLite, Spark and BigQuery :issue: 2273 ( #2178 ) [OmniSciDB] Add \"method\" parameter to load_data ( #2165 ) Add non-nullable info to schema output ( #2117 ) fillna and nullif implementations for OmnisciDB ( #2083 ) Add load_data to sqlalchemy's backends and fix database parameter for load/create/drop when database parameter is the same than the current database ( #1981 ) [OmniSciDB] Add support for within, d_fully_within and point ( #2125 ) OmniSciDB - Refactor DDL and Client; Add temporary parameter to create_table and \"force\" parameter to drop_view ( #2086 ) Create ExtractDayOfYear operation and add its support to Clickhouse, CSV, MySQL, OmniSciDB, Pandas, Parquet, PostgreSQL, PySpark, SQLite and Spark ( #2173 ) Implementations of Log Log2 Log10 for OmniSciDB backend ( #2095 ) Bugs Table expressions do not recognize inet datatype (Postgres backend) ( #2462 ) Table expressions do not recognize macaddr datatype (Postgres backend) ( #2461 ) Fix aggcontext.Summarize not always producing scalar (Pandas backend) ( #2410 ) Fix same window op with different window size on table lead to incorrect results for pyspark backend ( #2414 ) Fix same column with multiple aliases not showing properly in repr ( #2229 ) Fix reduction UDFs over ungrouped, bounded windows on Pandas backend ( #2395 ) FEAT: Support rolling window UDF with non numeric inputs for pandas backend. ( #2386 ) Fix scope get to use hashmap lookup instead of list lookup ( #2386 ) Fix equality behavior for Literal ops ( #2387 ) Fix analytic ops over ungrouped and unordered windows on Pandas backend ( #2376 ) Fix the covariance operator in the BigQuery backend. ( #2367 ) Update impala kerberos dependencies ( #2342 ) Added verbose logging to SQL backends ( #1320 ) Fix issue with sql_validate call to OmnisciDB. ( #2256 ) Add missing float types to pandas backend ( #2237 ) Allow group_by and order_by as window operation input in pandas backend ( #2252 ) Fix PySpark compiler error when elementwise UDF output_type is Decimal or Timestamp ( #2223 ) Fix interactive mode returning a expression instead of the value when used in Jupyter ( #2157 ) Fix PySpark error when doing alias after selection ( #2127 ) Fix millisecond issue for OmniSciDB :issue: 2167 , MySQL :issue: 2169 , PostgreSQL :issue: 2166 , Pandas :issue: 2168 , BigQuery :issue: 2273 backends ( #2170 ) [OmniSciDB] Fix TopK when used as filter ( #2134 ) Support Move ibis.HDFS , ibis.WebHDFS and ibis.hdfs_connect to ibis.impala.* ( #2497 ) Drop support to Python 3.6 ( #2288 ) Simplifying tests directories structure ( #2351 ) Update google-cloud-bigquery dependency minimum version to 1.12.0 ( #2304 ) Remove \"experimental\" mentions for OmniSciDB and Pandas backends ( #2234 ) Use an OmniSciDB image stable on CI ( #2244 ) Added fragment_size to table creation for OmniSciDB ( #2107 ) Added round() support for OmniSciDB ( #2096 ) Enabled cumulative ops support for OmniSciDB ( #2113 ) 1.3.0 (2020-02-27) Features Improve many arguments UDF performance in pandas backend. ( #2071 ) Add DenseRank, RowNumber, MinRank, Count, PercentRank/CumeDist window operations to OmniSciDB ( #1976 ) Introduce a top level vectorized UDF module (experimental). Implement element-wise UDF for pandas and PySpark backend. ( #2047 ) Add support for multi arguments window UDAF for the pandas backend ( #2035 ) Clean up window translation logic in pyspark backend ( #2004 ) Add docstring check to CI for an initial subset files ( #1996 ) Pyspark backend bounded windows ( #2001 ) Add more POSTGIS operations ( #1987 ) SQLAlchemy Default precision and scale to decimal types for PostgreSQL and MySQL ( #1969 ) Add support for array operations in PySpark backend ( #1983 ) Implement sort, if_null, null_if and notin for PySpark backend ( #1978 ) Add support for date/time operations in PySpark backend ( #1974 ) Add support for params, query_schema, and sql in PySpark backend ( #1973 ) Implement join for PySpark backend ( #1967 ) Validate AsOfJoin tolerance and attempt interval unit conversion ( #1952 ) filter for PySpark backend ( #1943 ) window operations for pyspark backend ( #1945 ) Implement IntervalSub for pandas backend ( #1951 ) PySpark backend string and column ops ( #1942 ) PySpark backend ( #1913 ) DDL support for Spark backend ( #1908 ) Support timezone aware arrow timestamps ( #1923 ) Add shapely geometries as input for literals ( #1860 ) Add geopandas as output for omniscidb ( #1858 ) Spark UDFs ( #1885 ) Add support for Postgres UDFs ( #1871 ) Spark tests ( #1830 ) Spark client ( #1807 ) Use pandas rolling apply to implement rows_with_max_lookback ( #1868 ) Bugs Pin \"clickhouse-driver\" to \">=0.1.3\" ( #2089 ) Fix load data stage for Linux CI ( #2069 ) Fix datamgr.py fail if IBIS_TEST_OMNISCIDB_DATABASE=omnisci ( #2057 ) Change pymapd connection parameter from \"session_id\" to \"sessionid\" ( #2041 ) Fix pandas backend to treat trailing_window preceding arg as window bound rather than window size (e.g. preceding=0 now indicates current row rather than window size 0) ( #2009 ) Fix handling of Array types in Postgres UDF ( #2015 ) Fix pydocstyle config ( #2010 ) Pinning clickhouse-driver<0.1.2 ( #2006 ) Fix CI log for database ( #1984 ) Fixes explain operation ( #1933 ) Fix incorrect assumptions about attached SQLite databases ( #1937 ) Upgrade to JDK11 ( #1938 ) sql method doesn't work when the query uses LIMIT clause ( #1903 ) Fix union implementation ( #1910 ) Fix failing com imports on master ( #1912 ) OmniSci/MapD - Fix reduction for bool ( #1901 ) Pass scope to grouping execution in the pandas backend ( #1899 ) Fix various Spark backend issues ( #1888 ) Make Nodes enforce the proper signature ( #1891 ) Fix according to bug in pd.to_datetime when passing the unit flag ( #1893 ) Fix small formatting buglet in PR merge tool ( #1883 ) Fix the case where we do not have an index when using preceding with intervals ( #1876 ) Fixed issues with geo data ( #1872 ) Remove -x from pytest call in linux CI ( #1869 ) Fix return type of Struct.from_tuples ( #1867 ) Support Add support to Python 3.8 ( #2066 ) Pin back version of isort ( #2079 ) Use user-defined port variables for Omnisci and PostgreSQL tests ( #2082 ) Change omniscidb image tag from v5.0.0 to v5.1.0 on docker-compose recipe ( #2077 ) [Omnisci] The same SRIDs for test_geo_spatial_binops ( #2051 ) Unpin rtree version ( #2078 ) Link pandas issues with xfail tests in pandas/tests/test_udf.py ( #2074 ) Disable Postgres tests on Windows CI. ( #2075 ) use conda for installation black and isort tools ( #2068 ) CI: Fix CI builds related to new pandas 1.0 compatibility ( #2061 ) Fix data map for int8 on OmniSciDB backend ( #2056 ) Add possibility to run tests for separate backend via make test BACKENDS=[YOUR BACKEND] ( #2052 ) Fix \"cudf\" import on OmniSciDB backend ( #2055 ) CI: Drop table only if it exists (OmniSciDB) ( #2050 ) Add initial documentation for OmniSciDB, MySQL, PySpark and SparkSQL backends, add initial documentation for geospatial methods and add links to Ibis wiki page ( #2034 ) Implement covariance for bigquery backend ( #2044 ) Add Spark to supported backends list ( #2046 ) Ping dependency of rtree to fix CI failure ( #2043 ) Drop support for Python 3.5 ( #2037 ) HTML escape column names and types in png repr. ( #2023 ) Add geospatial tutorial notebook ( #1991 ) Change omniscidb image tag from v4.7.0 to v5.0.0 on docker-compose recipe ( #2031 ) Pin \"semantic_version\" to \"<2.7\" in the docs build CI, fix \"builddoc\" and \"doc\" section inside \"Makefile\" and skip mysql tzinfo on CI to allow to run MySQL using docker container on a hard disk drive. ( #2030 ) Fixed impala start up issues ( #2012 ) cache all ops in translate() ( #1999 ) Add black step to CI ( #1988 ) Json UUID any ( #1962 ) Add log for database services ( #1982 ) Fix BigQuery backend fixture so batting and awards_players fixture re\u2026 ( #1972 ) Disable BigQuery explicitly in all/test_join.py ( #1971 ) Re-formatting all files using pre-commit hook ( #1963 ) Disable codecov report upload during CI builds ( #1961 ) Developer doc enhancements ( #1960 ) Missing geospatial ops for OmniSciDB ( #1958 ) Remove pandas deprecation warnings ( #1950 ) Add developer docs to get docker setup ( #1948 ) More informative IntegrityError on duplicate columns ( #1949 ) Improve geospatial literals and smoke tests ( #1928 ) PostGIS enhancements ( #1925 ) Rename mapd to omniscidb backend ( #1866 ) Fix failing BigQuery tests ( #1926 ) Added missing null literal op ( #1917 ) Update link to Presto website ( #1895 ) Removing linting from windows ( #1896 ) Fix link to NUMFOCUS CoC ( #1884 ) Added CoC section ( #1882 ) Remove pandas exception for rows_with_max_lookback ( #1859 ) Move CI pipelines to Azure ( #1856 ) 1.2.0 (2019-06-24) Features Add new geospatial functions to OmniSciDB backend ( #1836 ) allow pandas timedelta in rows_with_max_lookback ( #1838 ) Accept rows-with-max-lookback as preceding parameter ( #1825 ) PostGIS support ( #1787 ) Bugs Fix call to psql causing failing CI ( #1855 ) Fix nested array literal repr ( #1851 ) Fix repr of empty schema ( #1850 ) Add max_lookback to window replace and combine functions ( #1843 ) Partially revert #1758 ( #1837 ) Support Skip SQLAlchemy backend tests in connect method in backends.py ( #1847 ) Validate order_by when using rows_with_max_lookback window ( #1848 ) Generate release notes from commits ( #1845 ) Raise exception on backends where rows_with_max_lookback can't be implemented ( #1844 ) Tighter version spec for pytest ( #1840 ) Allow passing a branch to ci/feedstock.py ( #1826 ) 1.1.0 (2019-06-09) Features Conslidate trailing window functions ( #1809 ) Call to_interval when casting integers to intervals ( #1766 ) Add session feature to mapd client API ( #1796 ) Add min periods parameter to Window ( #1792 ) Allow strings for types in pandas UDFs ( #1785 ) Add missing date operations and struct field operation for the pandas backend ( #1790 ) Add window operations to the OmniSci backend ( #1771 ) Reimplement the pandas backend using topological sort ( #1758 ) Add marker for xfailing specific backends ( #1778 ) Enable window function tests where possible ( #1777 ) is_computable_arg dispatcher ( #1743 ) Added float32 and geospatial types for create table from schema ( #1753 ) Bugs Fix group_concat test and implementations ( #1819 ) Fix failing strftime tests on Python 3.7 ( #1818 ) Remove unnecessary (and erroneous in some cases) frame clauses ( #1757 ) Chained mutate operations are buggy ( #1799 ) Allow projections from joins to attempt fusion ( #1783 ) Fix Python 3.5 dependency versions ( #1798 ) Fix compatibility and bugs associated with pandas toposort reimplementation ( #1789 ) Fix outer_join generating LEFT join instead of FULL OUTER ( #1772 ) NullIf should enforce that its arguments are castable to a common type ( #1782 ) Fix conda create command in documentation ( #1775 ) Fix preceding and following with None ( #1765 ) PostgreSQL interval type not recognized ( #1661 ) Support Remove decorator hacks and add custom markers ( #1820 ) Add development deps to setup.py ( #1814 ) Fix design and developer docs ( #1805 ) Pin sphinx version to 2.0.1 ( #1810 ) Add pep8speaks integration ( #1793 ) Fix typo in UDF signature specification ( #1821 ) Clean up most xpassing tests ( #1779 ) Update omnisci container version ( #1781 ) Constrain PyMapD version to get passing builds ( #1776 ) Remove warnings and clean up some docstrings ( #1763 ) Add StringToTimestamp as unsupported ( #1638 ) Add isort pre-commit hooks ( #1759 ) Add Python 3.5 testing back to CI ( #1750 ) Re-enable CI for building step ( #1700 ) Update README reference to MapD to say OmniSci ( #1749 ) 1.0.0 (2019-03-26) Features Add black as a pre-commit hook ( #1735 ) Add support for the arbitrary aggregate in the mapd backend ( #1680 ) Add SQL method for the MapD backend ( #1731 ) Clean up merge PR script and use the actual merge feature of GitHub ( #1744 ) Add cross join to the pandas backend ( #1723 ) Implement default handler for multiple client pre_execute ( #1727 ) Implement BigQuery auth using pydata_google_auth ( #1728 ) Timestamp literal accepts a timezone parameter ( #1712 ) Remove support for passing integers to ibis.timestamp ( #1725 ) Add find_nodes to lineage ( #1704 ) Remove a bunch of deprecated APIs and clean up warnings ( #1714 ) Implement table distinct for the pandas backend ( #1716 ) Implement geospatial functions for MapD ( #1678 ) Implement geospatial types for MapD ( #1666 ) Add pre commit hook ( #1685 ) Getting started with mapd, mysql and pandas ( #1686 ) Support column names with special characters in mapd ( #1675 ) Allow operations to hide arguments from display ( #1669 ) Remove implicit ordering requirements in the PostgreSQL backend ( #1636 ) Add cross join operator to MapD ( #1655 ) Fix UDF bugs and add support for non-aggregate analytic functions ( #1637 ) Support string slicing with other expressions ( #1627 ) Publish the ibis roadmap ( #1618 ) Implement approx_median in BigQuery ( #1604 ) Make ibis node instances hashable ( #1611 ) Add range_window and trailing_range_window to docs ( #1608 ) Bugs Make dev/merge-pr.py script handle PR branches ( #1745 ) Fix NULLIF implementation for the pandas backend ( #1742 ) Fix casting to float in the MapD backend ( #1737 ) Fix testing for BigQuery after auth flow update ( #1741 ) Fix skipping for new BigQuery auth flow ( #1738 ) Fix bug in TableExpr.drop ( #1732 ) Filter the raw warning from newer pandas to support older pandas ( #1729 ) Fix BigQuery credentials link ( #1706 ) Add Union as an unsuppoted operation for MapD ( #1639 ) Fix visualizing an ibis expression when showing a selection after a table join ( #1705 ) Fix MapD exception for toDateTime ( #1659 ) Use == to compare strings ( #1701 ) Resolves joining with different column names ( #1647 ) Fix map get with compatible types ( #1643 ) Fixed where operator for MapD ( #1653 ) Remove parameters from mapd ( #1648 ) Make sure we cast when NULL is else in CASE expressions ( #1651 ) Fix equality ( #1600 ) Support Do not build universal wheels ( #1748 ) Remove tag prefix from versioneer ( #1747 ) Use releases to manage documentation ( #1746 ) Use cudf instead of pygdf ( #1694 ) Fix multiple CI issues ( #1696 ) Update mapd ci to v4.4.1 ( #1681 ) Enabled mysql CI on azure pipelines ( #1672 ) Remove support for Python 2 ( #1670 ) Fix flake8 and many other warnings ( #1667 ) Update README.md for impala and kudu ( #1664 ) Remove defaults as a channel from azure pipelines ( #1660 ) Fixes a very typo in the pandas/core.py docstring ( #1658 ) Unpin clickhouse-driver version ( #1657 ) Add test for reduction returning lists ( #1650 ) Fix Azure VM image name ( #1646 ) Updated MapD server-CI ( #1641 ) Add TableExpr.drop to API documentation ( #1645 ) Fix Azure deployment step ( #1642 ) Set up CI with Azure Pipelines ( #1640 ) Fix conda builds ( #1609 ) v0.14.0 (2018-08-23) This release brings refactored, more composable core components and rule system to ibis. We also focused quite heavily on the BigQuery backend this release. New Features Allow keyword arguments in Node subclasses ( #968 ) Splat args into Node subclasses instead of requiring a list ( #969 ) Add support for UNION in the BigQuery backend ( #1408 , #1409 ) Support for writing UDFs in BigQuery ( #1377 ). See the BigQuery UDF docs for more details. Support for cross-project expressions in the BigQuery backend. ( #1427 , #1428 ) Add strftime and to_timestamp support for BigQuery ( #1422 , #1410 ) Require google-cloud-bigquery >=1.0 ( #1424 ) Limited support for interval arithmetic in the pandas backend ( #1407 ) Support for subclassing TableExpr ( #1439 ) Fill out pandas backend operations ( #1423 ) Add common DDL APIs to the pandas backend ( #1464 ) Implement the sql method for BigQuery ( #1463 ) Add to_timestamp for BigQuery ( #1455 ) Add the mapd backend ( #1419 ) Implement range windows ( #1349 ) Support for map types in the pandas backend ( #1498 ) Add mean and sum for boolean types in BigQuery ( #1516 ) All recent versions of SQLAlchemy are now suppported ( #1384 ) Add support for NUMERIC types in the BigQuery backend ( #1534 ) Speed up grouped and rolling operations in the pandas backend ( #1549 ) Implement TimestampNow for BigQuery and pandas ( #1575 ) Bug Fixes Nullable property is now propagated through value types ( #1289 ) Implicit casting between signed and unsigned integers checks boundaries Fix precedence of case statement ( #1412 ) Fix handling of large timestamps ( #1440 ) Fix identical_to precedence ( #1458 ) Pandas 0.23 compatibility ( #1458 ) Preserve timezones in timestamp-typed literals ( #1459 ) Fix incorrect topological ordering of UNION expressions ( #1501 ) Fix projection fusion bug when attempting to fuse columns of the same name ( #1496 ) Fix output type for some decimal operations ( #1541 ) API Changes The previous, private rules API has been rewritten ( #1366 ) Defining input arguments for operations happens in a more readable fashion instead of the previous [input_type]{.title-ref} list. Removed support for async query execution (only Impala supported) Remove support for Python 3.4 ( #1326 ) BigQuery division defaults to using IEEE_DIVIDE ( #1390 ) Add tolerance parameter to asof_join ( #1443 ) v0.13.0 (2018-03-30) This release brings new backends, including support for executing against files, MySQL, Pandas user defined scalar and aggregations along with a number of bug fixes and reliability enhancements. We recommend that all users upgrade from earlier versions of Ibis. New Backends File Support for CSV & HDF5 ( #1165 , #1194 ) File Support for Parquet Format ( #1175 , #1194 ) Experimental support for MySQL thanks to \\@kszucs ( #1224 ) New Features Support for Unsigned Integer Types ( #1194 ) Support for Interval types and expressions with support for execution on the Impala and Clickhouse backends ( #1243 ) Isnan, isinf operations for float and double values ( #1261 ) Support for an interval with a quarter period ( #1259 ) ibis.pandas.from_dataframe convenience function ( #1155 ) Remove the restriction on ROW_NUMBER() requiring it to have an ORDER BY clause ( #1371 ) Add .get() operation on a Map type ( #1376 ) Allow visualization of custom defined expressions Add experimental support for pandas UDFs/UDAFs ( #1277 ) Functions can be used as groupby keys ( #1214 , #1215 ) Generalize the use of the where parameter to reduction operations ( #1220 ) Support for interval operations thanks to \\@kszucs ( #1243 , #1260 , #1249 ) Support for the PARTITIONTIME column in the BigQuery backend ( #1322 ) Add arbitrary() method for selecting the first non null value in a column ( #1230 , #1309 ) Windowed MultiQuantile operation in the pandas backend thanks to \\@DiegoAlbertoTorres ( #1343 ) Rules for validating table expressions thanks to \\@DiegoAlbertoTorres ( #1298 ) Complete end-to-end testing framework for all supported backends ( #1256 ) contains / not contains now supported in the pandas backend ( #1210 , #1211 ) CI builds are now reproducible locally thanks to \\@kszucs ( #1121 , #1237 , #1255 , #1311 ) isnan / isinf operations thanks to \\@kszucs ( #1261 ) Framework for generalized dtype and schema inference, and implicit casting thanks to \\@kszucs ( #1221 , #1269 ) Generic utilities for expression traversal thanks to \\@kszucs ( #1336 ) day_of_week API ( #306 , #1047 ) Design documentation for ibis ( #1351 ) Bug Fixes Unbound parameters were failing in the simple case of a ibis.expr.types.TableExpr.mutate call with no operation ( #1378 ) Fix parameterized subqueries ( #1300 , #1331 , #1303 , #1378 ) Fix subquery extraction, which wasn\\'t happening in topological order ( #1342 ) Fix parenthesization if isnull ( #1307 ) Calling drop after mutate did not work ( #1296 , #1299 ) SQLAlchemy backends were missing an implementation of ibis.expr.operations.NotContains . Support REGEX_EXTRACT in PostgreSQL 10 ( #1276 , #1278 ) API Changes Fixing #1378 required the removal of the name parameter to the ibis.param function. Use the ibis.expr.types.Expr.name method instead. v0.12.0 (2017-10-28) This release brings Clickhouse and BigQuery SQL support along with a number of bug fixes and reliability enhancements. We recommend that all users upgrade from earlier versions of Ibis. New Backends BigQuery backend ( #1170 ), thanks to \\@tsdlovell. Clickhouse backend ( #1127 ), thanks to \\@kszucs. New Features Add support for Binary data type ( #1183 ) Allow users of the BigQuery client to define their own API proxy classes ( #1188 ) Add support for HAVING in the pandas backend ( #1182 ) Add struct field tab completion ( #1178 ) Add expressions for Map/Struct types and columns ( #1166 ) Support Table.asof_join ( #1162 ) Allow right side of arithmetic operations to take over ( #1150 ) Add a data_preload step in pandas backend ( #1142 ) expressions in join predicates in the pandas backend ( #1138 ) Scalar parameters ( #1075 ) Limited window function support for pandas ( #1083 ) Implement Time datatype ( #1105 ) Implement array ops for pandas ( #1100 ) support for passing multiple quantiles in .quantile() ( #1094 ) support for clip and quantile ops on DoubleColumns ( #1090 ) Enable unary math operations for pandas, sqlite ( #1071 ) Enable casting from strings to temporal types ( #1076 ) Allow selection of whole tables in pandas joins ( #1072 ) Implement comparison for string vs date and timestamp types ( #1065 ) Implement isnull and notnull for pandas ( #1066 ) Allow like operation to accept a list of conditions to match ( #1061 ) Add a pre_execute step in pandas backend ( #1189 ) Bug Fixes Remove global expression caching to ensure repeatable code generation ( #1179 , #1181 ) Fix ORDER BY generation without a GROUP BY ( #1180 , #1181 ) Ensure that ~ibis.expr.datatypes.DataType and subclasses hash properly ( #1172 ) Ensure that the pandas backend can deal with unary operations in groupby ( #1182 ) Incorrect impala code generated for NOT with complex argument ( #1176 ) BUG/CLN: Fix predicates on Selections on Joins ( #1149 ) Don\\'t use SET LOCAL to allow redshift to work ( #1163 ) Allow empty arrays as arguments ( #1154 ) Fix column renaming in groupby keys ( #1151 ) Ensure that we only cast if timezone is not None ( #1147 ) Fix location of conftest.py ( #1107 ) TST/Make sure we drop tables during postgres testing ( #1101 ) Fix misleading join error message ( #1086 ) BUG/TST: Make hdfs an optional dependency ( #1082 ) Memoization should include expression name where available ( #1080 ) Performance Enhancements Speed up imports ( #1074 ) Fix execution perf of groupby and selection ( #1073 ) Use normalize for casting to dates in pandas ( #1070 ) Speed up pandas groupby ( #1067 ) Contributors The following people contributed to the 0.12.0 release : $ git shortlog -sn --no-merges v0.11.2..v0.12.0 63 Phillip Cloud 8 Jeff Reback 2 Kriszti\u00e1n Sz\u0171cs 2 Tory Haavik 1 Anirudh 1 Szucs Krisztian 1 dlovell 1 kwangin 0.11.0 (2017-06-28) This release brings initial Pandas backend support along with a number of bug fixes and reliability enhancements. We recommend that all users upgrade from earlier versions of Ibis. New Features Experimental pandas backend to allow execution of ibis expression against pandas DataFrames Graphviz visualization of ibis expressions. Implements _repr_png_ for Jupyter Notebook functionality Ability to create a partitioned table from an ibis expression Support for missing operations in the SQLite backend: sqrt, power, variance, and standard deviation, regular expression functions, and missing power support for PostgreSQL Support for schemas inside databases with the PostgreSQL backend Appveyor testing on core ibis across all supported Python versions Add year / month / day methods to date types Ability to sort, group by and project columns according to positional index rather than only by name Added a type parameter to ibis.literal to allow user specification of literal types Bug Fixes Fix broken conda recipe Fix incorrectly typed fillna operation Fix postgres boolean summary operations Fix kudu support to reflect client API Changes Fix equality of nested types and construction of nested types when the value type is specified as a string API Changes Deprecate passing integer values to the ibis.timestamp literal constructor, this will be removed in 0.12.0 Added the admin_timeout parameter to the kudu client connect function Contributors $ git shortlog --summary --numbered v0.10.0..v0.11.0 58 Phillip Cloud 1 Greg Rahn 1 Marius van Niekerk 1 Tarun Gogineni 1 Wes McKinney 0.8 (2016-05-19) This release brings initial PostgreSQL backend support along with a number of critical bug fixes and usability improvements. As several correctness bugs with the SQL compiler were fixed, we recommend that all users upgrade from earlier versions of Ibis. New Features Initial PostgreSQL backend contributed by Phillip Cloud. Add groupby as an alias for group_by to table expressions Bug Fixes Fix an expression error when filtering based on a new field Fix Impala\\'s SQL compilation of using OR with compound filters Various fixes with the having(...) function in grouped table expressions Fix CTE ( WITH ) extraction inside UNION ALL expressions. Fix ImportError on Python 2 when mock library not installed API Changes The deprecated ibis.impala_connect and ibis.make_client APIs have been removed 0.7 (2016-03-16) This release brings initial Kudu-Impala integration and improved Impala and SQLite support, along with several critical bug fixes. New Features Apache Kudu (incubating) integration for Impala users. See the blog post for now. Will add some documentation here when possible. Add use_https option to ibis.hdfs_connect for WebHDFS connections in secure (Kerberized) clusters without SSL enabled. Correctly compile aggregate expressions involving multiple subqueries. To explain this last point in more detail, suppose you had: table = ibis . table ([( 'flag' , 'string' ), ( 'value' , 'double' )], 'tbl' ) flagged = table [ table . flag == '1' ] unflagged = table [ table . flag == '0' ] fv = flagged . value uv = unflagged . value expr = ( fv . mean () / fv . sum ()) - ( uv . mean () / uv . sum ()) The last expression now generates the correct Impala or SQLite SQL: SELECT t0 . ` tmp ` - t1 . ` tmp ` AS ` tmp ` FROM ( SELECT avg ( ` value ` ) / sum ( ` value ` ) AS ` tmp ` FROM tbl WHERE ` flag ` = '1' ) t0 CROSS JOIN ( SELECT avg ( ` value ` ) / sum ( ` value ` ) AS ` tmp ` FROM tbl WHERE ` flag ` = '0' ) t1 Bug Fixes CHAR(n) and VARCHAR(n) Impala types now correctly map to Ibis string expressions Fix inappropriate projection-join-filter expression rewrites resulting in incorrect generated SQL. ImpalaClient.create_table correctly passes STORED AS PARQUET for format='parquet' . Fixed several issues with Ibis dependencies (impyla, thriftpy, sasl, thrift_sasl), especially for secure clusters. Upgrading will pull in these new dependencies. Do not fail in ibis.impala.connect when trying to create the temporary Ibis database if no HDFS connection passed. Fix join predicate evaluation bug when column names overlap with table attributes. Fix handling of fully-materialized joins (aka select * joins) in SQLAlchemy / SQLite. Contributors Thank you to all who contributed patches to this release. $ git log v0.6.0..v0.7.0 --pretty=format:%aN | sort | uniq -c | sort -rn 21 Wes McKinney 1 Uri Laserson 1 Kristopher Overholt 0.6 (2015-12-01) This release brings expanded pandas and Impala integration, including support for managing partitioned tables in Impala. See the new Ibis for Impala Users guide for more on using Ibis with Impala. The Ibis for SQL Programmers guide also was written since the 0.5 release. This release also includes bug fixes affecting generated SQL correctness. All users should upgrade as soon as possible. New Features New integrated Impala functionality. See Ibis for Impala Users for more details on these things. Improved Impala-pandas integration. Create tables or insert into existing tables from pandas DataFrame objects. Partitioned table metadata management API. Add, drop, alter, and insert into table partitions. Add is_partitioned property to ImpalaTable . Added support for LOAD DATA DDL using the load_data function, also supporting partitioned tables. Modify table metadata (location, format, SerDe properties etc.) using ImpalaTable.alter Interrupting Impala expression execution with Control-C will attempt to cancel the running query with the server. Set the compression codec (e.g. snappy) used with ImpalaClient.set_compression_codec . Get and set query options for a client session with ImpalaClient.get_options and ImpalaClient.set_options . Add ImpalaTable.metadata method that parses the output of the DESCRIBE FORMATTED DDL to simplify table metadata inspection. Add ImpalaTable.stats and ImpalaTable.column_stats to see computed table and partition statistics. Add CHAR and VARCHAR handling Add refresh , invalidate_metadata DDL options and add incremental option to compute_stats for COMPUTE INCREMENTAL STATS . Add substitute method for performing multiple value substitutions in an array or scalar expression. Division is by default true division like Python 3 for all numeric data. This means for SQL systems that use C-style division semantics, the appropriate CAST will be automatically inserted in the generated SQL. Easier joins on tables with overlapping column names. See Ibis for SQL Programmers . Expressions like string_expr[:3] now work as expected. Add coalesce instance method to all value expressions. Passing limit=None to the execute method on expressions disables any default row limits. API Changes ImpalaTable.rename no longer mutates the calling table expression. Contributors $ git log v0.5.0..v0.6.0 --pretty=format:%aN | sort | uniq -c | sort -rn 46 Wes McKinney 3 Uri Laserson 1 Phillip Cloud 1 mariusvniekerk 1 Kristopher Overholt 0.5 (2015-09-10) Highlights in this release are the SQLite, Python 3, Impala UDA support, and an asynchronous execution API. There are also many usability improvements, bug fixes, and other new features. New Features SQLite client and built-in function support Ibis now supports Python 3.4 as well as 2.6 and 2.7 Ibis can utilize Impala user-defined aggregate (UDA) functions SQLAlchemy-based translation toolchain to enable more SQL engines having SQLAlchemy dialects to be supported Many window function usability improvements (nested analytic functions and deferred binding conveniences) More convenient aggregation with keyword arguments in aggregate functions Built preliminary wrapper API for MADLib-on-Impala Add var and std aggregation methods and support in Impala Add nullifzero numeric method for all SQL engines Add rename method to Impala tables (for renaming tables in the Hive metastore) Add close method to ImpalaClient for session cleanup (#533) Add relabel method to table expressions Add insert method to Impala tables Add compile and verify methods to all expressions to test compilation and ability to compile (since many operations are unavailable in SQLite, for example) API Changes Impala Ibis client creation now uses only ibis.impala.connect , and ibis.make_client has been deprecated Contributors $ git log v0.4.0..v0.5.0 --pretty=format:%aN | sort | uniq -c | sort -rn 55 Wes McKinney 9 Uri Laserson 1 Kristopher Overholt 0.4 (2015-08-14) New Features Add tooling to use Impala C++ scalar UDFs within Ibis (#262, #195) Support and testing for Kerberos-enabled secure HDFS clusters Many table functions can now accept functions as parameters (invoked on the calling table) to enhance composability and emulate late-binding semantics of languages (like R) that have non-standard evaluation (#460) Add any , all , notany , and notall reductions on boolean arrays, as well as cumany and cumall Using topk now produces an analytic expression that is executable (as an aggregation) but can also be used as a filter as before (#392, #91) Added experimental database object \\\"usability layer\\\", see ImpalaClient.database . Add TableExpr.info Add compute_stats API to table expressions referencing physical Impala tables Add explain method to ImpalaClient to show query plan for an expression Add chmod and chown APIs to HDFS interface for superusers Add convert_base method to strings and integer types Add option to ImpalaClient.create_table to create empty partitioned tables ibis.cross_join can now join more than 2 tables at once Add ImpalaClient.raw_sql method for running naked SQL queries ImpalaClient.insert now validates schemas locally prior to sending query to cluster, for better usability. Add conda installation recipes Contributors $ git log v0.3.0..v0.4.0 --pretty=format:%aN | sort | uniq -c | sort -rn 38 Wes McKinney 9 Uri Laserson 2 Meghana Vuyyuru 2 Kristopher Overholt 1 Marius van Niekerk 0.3 (2015-07-20) First public release. See http://ibis-project.org for more. New Features Implement window / analytic function support Enable non-equijoins (join clauses with operations other than == ). Add remaining string functions supported by Impala. Add pipe method to tables (hat-tip to the pandas dev team). Add mutate convenience method to tables. Fleshed out WebHDFS implementations: get/put directories, move files, etc. See the full HDFS API . Add truncate method for timestamp values ImpalaClient can execute scalar expressions not involving any table. Can also create internal Impala tables with a specific HDFS path. Make Ibis\\'s temporary Impala database and HDFS paths configurable (see ibis.options ). Add truncate_table function to client (if the user\\'s Impala cluster supports it). Python 2.6 compatibility Enable Ibis to execute concurrent queries in multithreaded applications (earlier versions were not thread-safe). Test data load script in scripts/load_test_data.py Add an internal operation type signature API to enhance developer productivity. Contributors $ git log v0.2.0..v0.3.0 --pretty=format:%aN | sort | uniq -c | sort -rn 59 Wes McKinney 29 Uri Laserson 4 Isaac Hodes 2 Meghana Vuyyuru 0.2 (2015-06-16) New Features insert method on Ibis client for inserting data into existing tables. parquet_file , delimited_file , and avro_file client methods for querying datasets not yet available in Impala New ibis.hdfs_connect method and HDFS client API for WebHDFS for writing files and directories to HDFS New timedelta API and improved timestamp data support New bucket and histogram methods on numeric expressions New category logical datatype for handling bucketed data, among other things Add summary API to numeric expressions Add value_counts convenience API to array expressions New string methods like , rlike , and contains for fuzzy and regex searching Add options.verbose option and configurable options.verbose_log callback function for improved query logging and visibility Support for new SQL built-in functions ibis.coalesce ibis.greatest and ibis.least ibis.where for conditional logic (see also ibis.case and ibis.cases ) nullif method on value expressions ibis.now New aggregate functions: approx_median , approx_nunique , and group_concat where argument in aggregate functions Add having method to group_by intermediate object Added group-by convenience table.group_by(exprs).COLUMN_NAME.agg_function() Add default expression names to most aggregate functions New Impala database client helper methods create_database drop_database exists_database list_databases set_database Client list_tables searching / listing method Add add , sub , and other explicit arithmetic methods to value expressions API Changes New Ibis client and Impala connection workflow. Client now combined from an Impala connection and an optional HDFS connection Bug Fixes Numerous expression API bug fixes and rough edges fixed Contributors $ git log v0.1.0..v0.2.0 --pretty=format:%aN | sort | uniq -c | sort -rn 71 Wes McKinney 1 Juliet Hougland 1 Isaac Hodes 0.1 (2015-03-26) First Ibis release. Expression DSL design and type system Expression to ImpalaSQL compiler toolchain Impala built-in function wrappers $ git log 84d0435..v0.1.0 --pretty=format:%aN | sort | uniq -c | sort -rn 78 Wes McKinney 1 srus 1 Henry Robinson","title":"Release Notes"},{"location":"release_notes/#release-notes","text":"","title":"Release Notes"},{"location":"release_notes/#211-2022-01-12","text":"","title":"2.1.1 (2022-01-12)"},{"location":"release_notes/#bug-fixes","text":"setup.py: set the correct version number for 2.1.0 ( f3d267b )","title":"Bug Fixes"},{"location":"release_notes/#210-2022-01-12","text":"","title":"2.1.0 (2022-01-12)"},{"location":"release_notes/#bug-fixes_1","text":"consider all packages' entry points ( b495cf6 ) datatypes: infer bytes literal as binary #2915 ( #3124 ) ( 887efbd ) deps: bump minimum dask version to 2021.10.0 ( e6b5c09 ) deps: constrain numpy to ensure wheels are used on windows ( 70c308b ) deps: update dependency clickhouse-driver to ^0.1 || ^0.2.0 ( #3061 ) ( a839d54 ) deps: update dependency geoalchemy2 to >=0.6,<0.11 ( 4cede9d ) deps: update dependency pyarrow to v6 ( #3092 ) ( 61e52b5 ) don't force backends to override do_connect until 3.0.0 ( 4b46973 ) execute materialized joins in the pandas and dask backends ( #3086 ) ( 9ed937a ) literal: allow creating ibis literal with uuid ( #3131 ) ( b0f4f44 ) restore the ability to have more than two option levels ( #3151 ) ( fb4a944 ) sqlalchemy: fix correlated subquery compilation ( 43b9010 ) sqlite: defer db connection until needed ( #3127 ) ( 5467afa ), closes #64","title":"Bug Fixes"},{"location":"release_notes/#features","text":"allow column_of to take a column expression ( dbc34bb ) ci: More readable workflow job titles ( #3111 ) ( d8fd7d9 ) datafusion: initial implementation for Arrow Datafusion backend ( 3a67840 ), closes #2627 datafusion: initial implementation for Arrow Datafusion backend ( 75876d9 ), closes #2627 make dayofweek impls conform to pandas semantics ( #3161 ) ( 9297828 )","title":"Features"},{"location":"release_notes/#reverts","text":"\"ci: install gdal for fiona\" ( 8503361 )","title":"Reverts"},{"location":"release_notes/#200-2021-10-06","text":"","title":"2.0.0 (2021-10-06)"},{"location":"release_notes/#features_1","text":"Serialization-deserialization of Node via pickle is now byte compatible between different processes ( #2938 ) Support joining on different columns in ClickHouse backend ( #2916 ) Support summarization of empty data in Pandas backend ( #2908 ) Unify implementation of fillna and isna in Pyspark backend ( #2882 ) Support binary operation with Timedelta in Pyspark backend ( #2873 ) Add group_concat operation for Clickhouse backend ( #2839 ) Support comparison of ColumnExpr to timestamp literal ( #2808 ) Make op schema a cached property ( #2805 ) Implement .insert() for SQLAlchemy backends ( #2613 , #2613 ) Infer categorical and decimal Series to more specific Ibis types in Pandas backend ( #2792 ) Add startswith and endswith operations ( #2790 ) Allow more flexible return type for UDFs ( #2776 , #2797 ) Implement Clip in the Pyspark backend ( #2779 ) Use ndarray as array representation in Pandas backend ( #2753 ) Support Spark filter with window operation ( #2687 ) Support context adjustment for udfs for pandas backend ( #2646 ) Add auth_local_webserver , auth_external_data , and auth_cache parameters to BigQuery connect method. Set auth_local_webserver to use a local server instead of copy-pasting an authorization code. Set auth_external_data to true to request additional scopes required to query Google Drive and Sheets. Set auth_cache to reauth or none to force reauthentication. ( #2655 ) Add bit_and , bit_or , and bit_xor integer column aggregates (BigQuery and MySQL backends) ( #2641 ) Backends are defined as entry points ( #2379 ) Add ibis.array for creating array expressions ( #2615 ) Implement Not operation in PySpark backend ( #2607 ) Added support for case/when in PySpark backend ( #2610 ) Add support for np.array as literals for backends that already support lists as literals ( #2603 )","title":"Features"},{"location":"release_notes/#bugs","text":"Fix data races in impala connection pool accounting ( #2991 ) Fix null literal compilation in the Clickhouse backend ( #2985 ) Fix order of limit and offset parameters in the Clickhouse backend ( #2984 ) Replace equals operation for geospatial datatype to geo_equals ( #2956 ) Fix .drop(fields). The argument can now be either a list of strings or a string. ( #2829 ) Fix projection on differences and intersections for SQL backends ( #2845 ) Backends are loaded in a lazy way, so third-party backends can import Ibis without circular imports ( #2827 ) Disable aggregation optimization due to N squared performance ( #2830 ) Fix .cast() to array outputting list instead of np.array in Pandas backend ( #2821 ) Fix aggregation with mixed reduction datatypes (array + scalar) on Dask backend ( #2820 ) Fix error when using reduction UDF that returns np.array in a grouped aggregation ( #2770 ) Fix time context trimming error for multi column udfs in pandas backend ( #2712 ) Fix error during compilation of range_window in base_sql backends (:issue: 2608 ) ( #2710 ) Fix wrong row indexing in the result for 'window after filter' for timecontext adjustment ( #2696 ) Fix aggregate exploding the output of Reduction ops that return a list/ndarray ( #2702 ) Fix issues with context adjustment for filter with PySpark backend ( #2693 ) Add temporary struct col in pyspark backend to ensure that UDFs are executed only once ( #2657 ) Fix BigQuery connect bug that ignored project ID parameter ( #2588 ) Fix overwrite logic to account for DestructColumn inside mutate API ( #2636 ) Fix fusion optimization bug that incorrectly changes operation order ( #2635 ) Fixes a NPE issue with substr in PySpark backend ( #2610 ) Fixes binary data type translation into BigQuery bytes data type ( #2354 ) Make StructValue picklable ( #2577 )","title":"Bugs"},{"location":"release_notes/#support","text":"Improvement of the backend API. The former Client subclasses have been replaced by a Backend class that must subclass ibis.backends.base.BaseBackend . The BaseBackend class contains abstract methods for the minimum subset of methods that backends must implement, and their signatures have been standardized across backends. The Ibis compiler has been refactored, and backends don't need to implement all compiler classes anymore if the default works for them. Only a subclass of ibis.backends.base.sql.compiler.Compiler is now required. Backends now need to register themselves as entry points. ( #2678 ) Deprecate exists_table(table) in favor of table in list_tables() ( #2905 ) Remove handwritten type parser; parsing errors that were previously IbisTypeError are now parsy.ParseError . parsy is now a hard requirement. ( #2977 ) Methods current_database and list_databases raise an exception for backends that do not support databases ( #2962 ) Method set_database has been deprecated, in favor of creating a new connection to a different database ( #2913 ) Removed log method of clients, in favor of verbose_log option ( #2914 ) Output of Client.version returned as a string, instead of a setuptools Version ( #2883 ) Deprecated list_schemas in SQLAlchemy backends in favor of list_databases ( #2862 ) Deprecated ibis.<backend>.verify() in favor of capturing exception in ibis.<backend>.compile() ( #2865 ) Simplification of data fetching. Backends don't need to implement Query anymore ( #2789 ) Move BigQuery backend to a separate repository <https://github.com/ibis-project/ibis-bigquery> _. The backend will be released separately, use pip install ibis-bigquery or conda install ibis-bigquery to install it, and then use as before. ( #2665 ) Supporting SQLAlchemy 1.4, and requiring minimum 1.3 ( #2689 ) Namespace time_col config, fix type check for trim_with_timecontext for pandas window execution ( #2680 ) Remove deprecated ibis.HDFS , ibis.WebHDFS and ibis.hdfs_connect ( #2505 )","title":"Support"},{"location":"release_notes/#140-2020-11-07","text":"","title":"1.4.0 (2020-11-07)"},{"location":"release_notes/#features_2","text":"Add Struct.from_dict ( #2514 ) Add hash and hashbytes support for BigQuery backend ( #2310 ) Support reduction UDF without groupby to return multiple columns for Pandas backend ( #2511 ) Support analytic and reduction UDF to return multiple columns for Pandas backend ( #2487 ) Support elementwise UDF to return multiple columns for Pandas and PySpark backend ( #2473 ) FEAT: Support Ibis interval for window in pyspark backend ( #2409 ) Use Scope class for scope in pyspark backend ( #2402 ) Add PySpark support for ReductionVectorizedUDF ( #2366 ) Add time context in scope in execution for pandas backend ( #2306 ) Add start_point and end_point to PostGIS backend. ( #2081 ) Add set difference to general ibis api ( #2347 ) Add rowid expression, supported by SQLite and OmniSciDB ( #2251 ) Add intersection to general ibis api ( #2230 ) Add application_name argument to ibis.bigquery.connect to allow attributing Google API requests to projects that use Ibis. ( #2303 ) Add support for casting category dtype in pandas backend ( #2285 ) Add support for Union in the PySpark backend ( #2270 ) Add support for implementign custom window object for pandas backend ( #2260 ) Implement two level dispatcher for execute_node ( #2246 ) Add ibis.pandas.trace module to log time and call stack information. ( #2233 ) Validate that the output type of a UDF is a single element ( #2198 ) ZeroIfNull and NullIfZero implementation for OmniSciDB ( #2186 ) IsNan implementation for OmniSciDB ( #2093 ) [OmnisciDB] Support add_columns and drop_columns for OmnisciDB table ( #2094 ) Create ExtractQuarter operation and add its support to Clickhouse, CSV, Impala, MySQL, OmniSciDB, Pandas, Parquet, PostgreSQL, PySpark, SQLite and Spark ( #2175 ) Add translation rules for isnull() and notnull() for pyspark backend ( #2126 ) Add window operations support to SQLite ( #2232 ) Implement read_csv for omniscidb backend ( #2062 ) [OmniSciDB] Add support to week extraction ( #2171 ) Date, DateDiff and TimestampDiff implementations for OmniSciDB ( #2097 ) Create ExtractWeekOfYear operation and add its support to Clickhouse, CSV, MySQL, Pandas, Parquet, PostgreSQL, PySpark and Spark ( #2177 ) Add initial support for ibis.random function ( #2060 ) Added epoch_seconds extraction operation to Clickhouse, CSV, Impala, MySQL, OmniSciDB, Pandas, Parquet, PostgreSQL, PySpark, SQLite, Spark and BigQuery :issue: 2273 ( #2178 ) [OmniSciDB] Add \"method\" parameter to load_data ( #2165 ) Add non-nullable info to schema output ( #2117 ) fillna and nullif implementations for OmnisciDB ( #2083 ) Add load_data to sqlalchemy's backends and fix database parameter for load/create/drop when database parameter is the same than the current database ( #1981 ) [OmniSciDB] Add support for within, d_fully_within and point ( #2125 ) OmniSciDB - Refactor DDL and Client; Add temporary parameter to create_table and \"force\" parameter to drop_view ( #2086 ) Create ExtractDayOfYear operation and add its support to Clickhouse, CSV, MySQL, OmniSciDB, Pandas, Parquet, PostgreSQL, PySpark, SQLite and Spark ( #2173 ) Implementations of Log Log2 Log10 for OmniSciDB backend ( #2095 )","title":"Features"},{"location":"release_notes/#bugs_1","text":"Table expressions do not recognize inet datatype (Postgres backend) ( #2462 ) Table expressions do not recognize macaddr datatype (Postgres backend) ( #2461 ) Fix aggcontext.Summarize not always producing scalar (Pandas backend) ( #2410 ) Fix same window op with different window size on table lead to incorrect results for pyspark backend ( #2414 ) Fix same column with multiple aliases not showing properly in repr ( #2229 ) Fix reduction UDFs over ungrouped, bounded windows on Pandas backend ( #2395 ) FEAT: Support rolling window UDF with non numeric inputs for pandas backend. ( #2386 ) Fix scope get to use hashmap lookup instead of list lookup ( #2386 ) Fix equality behavior for Literal ops ( #2387 ) Fix analytic ops over ungrouped and unordered windows on Pandas backend ( #2376 ) Fix the covariance operator in the BigQuery backend. ( #2367 ) Update impala kerberos dependencies ( #2342 ) Added verbose logging to SQL backends ( #1320 ) Fix issue with sql_validate call to OmnisciDB. ( #2256 ) Add missing float types to pandas backend ( #2237 ) Allow group_by and order_by as window operation input in pandas backend ( #2252 ) Fix PySpark compiler error when elementwise UDF output_type is Decimal or Timestamp ( #2223 ) Fix interactive mode returning a expression instead of the value when used in Jupyter ( #2157 ) Fix PySpark error when doing alias after selection ( #2127 ) Fix millisecond issue for OmniSciDB :issue: 2167 , MySQL :issue: 2169 , PostgreSQL :issue: 2166 , Pandas :issue: 2168 , BigQuery :issue: 2273 backends ( #2170 ) [OmniSciDB] Fix TopK when used as filter ( #2134 )","title":"Bugs"},{"location":"release_notes/#support_1","text":"Move ibis.HDFS , ibis.WebHDFS and ibis.hdfs_connect to ibis.impala.* ( #2497 ) Drop support to Python 3.6 ( #2288 ) Simplifying tests directories structure ( #2351 ) Update google-cloud-bigquery dependency minimum version to 1.12.0 ( #2304 ) Remove \"experimental\" mentions for OmniSciDB and Pandas backends ( #2234 ) Use an OmniSciDB image stable on CI ( #2244 ) Added fragment_size to table creation for OmniSciDB ( #2107 ) Added round() support for OmniSciDB ( #2096 ) Enabled cumulative ops support for OmniSciDB ( #2113 )","title":"Support"},{"location":"release_notes/#130-2020-02-27","text":"","title":"1.3.0 (2020-02-27)"},{"location":"release_notes/#features_3","text":"Improve many arguments UDF performance in pandas backend. ( #2071 ) Add DenseRank, RowNumber, MinRank, Count, PercentRank/CumeDist window operations to OmniSciDB ( #1976 ) Introduce a top level vectorized UDF module (experimental). Implement element-wise UDF for pandas and PySpark backend. ( #2047 ) Add support for multi arguments window UDAF for the pandas backend ( #2035 ) Clean up window translation logic in pyspark backend ( #2004 ) Add docstring check to CI for an initial subset files ( #1996 ) Pyspark backend bounded windows ( #2001 ) Add more POSTGIS operations ( #1987 ) SQLAlchemy Default precision and scale to decimal types for PostgreSQL and MySQL ( #1969 ) Add support for array operations in PySpark backend ( #1983 ) Implement sort, if_null, null_if and notin for PySpark backend ( #1978 ) Add support for date/time operations in PySpark backend ( #1974 ) Add support for params, query_schema, and sql in PySpark backend ( #1973 ) Implement join for PySpark backend ( #1967 ) Validate AsOfJoin tolerance and attempt interval unit conversion ( #1952 ) filter for PySpark backend ( #1943 ) window operations for pyspark backend ( #1945 ) Implement IntervalSub for pandas backend ( #1951 ) PySpark backend string and column ops ( #1942 ) PySpark backend ( #1913 ) DDL support for Spark backend ( #1908 ) Support timezone aware arrow timestamps ( #1923 ) Add shapely geometries as input for literals ( #1860 ) Add geopandas as output for omniscidb ( #1858 ) Spark UDFs ( #1885 ) Add support for Postgres UDFs ( #1871 ) Spark tests ( #1830 ) Spark client ( #1807 ) Use pandas rolling apply to implement rows_with_max_lookback ( #1868 )","title":"Features"},{"location":"release_notes/#bugs_2","text":"Pin \"clickhouse-driver\" to \">=0.1.3\" ( #2089 ) Fix load data stage for Linux CI ( #2069 ) Fix datamgr.py fail if IBIS_TEST_OMNISCIDB_DATABASE=omnisci ( #2057 ) Change pymapd connection parameter from \"session_id\" to \"sessionid\" ( #2041 ) Fix pandas backend to treat trailing_window preceding arg as window bound rather than window size (e.g. preceding=0 now indicates current row rather than window size 0) ( #2009 ) Fix handling of Array types in Postgres UDF ( #2015 ) Fix pydocstyle config ( #2010 ) Pinning clickhouse-driver<0.1.2 ( #2006 ) Fix CI log for database ( #1984 ) Fixes explain operation ( #1933 ) Fix incorrect assumptions about attached SQLite databases ( #1937 ) Upgrade to JDK11 ( #1938 ) sql method doesn't work when the query uses LIMIT clause ( #1903 ) Fix union implementation ( #1910 ) Fix failing com imports on master ( #1912 ) OmniSci/MapD - Fix reduction for bool ( #1901 ) Pass scope to grouping execution in the pandas backend ( #1899 ) Fix various Spark backend issues ( #1888 ) Make Nodes enforce the proper signature ( #1891 ) Fix according to bug in pd.to_datetime when passing the unit flag ( #1893 ) Fix small formatting buglet in PR merge tool ( #1883 ) Fix the case where we do not have an index when using preceding with intervals ( #1876 ) Fixed issues with geo data ( #1872 ) Remove -x from pytest call in linux CI ( #1869 ) Fix return type of Struct.from_tuples ( #1867 )","title":"Bugs"},{"location":"release_notes/#support_2","text":"Add support to Python 3.8 ( #2066 ) Pin back version of isort ( #2079 ) Use user-defined port variables for Omnisci and PostgreSQL tests ( #2082 ) Change omniscidb image tag from v5.0.0 to v5.1.0 on docker-compose recipe ( #2077 ) [Omnisci] The same SRIDs for test_geo_spatial_binops ( #2051 ) Unpin rtree version ( #2078 ) Link pandas issues with xfail tests in pandas/tests/test_udf.py ( #2074 ) Disable Postgres tests on Windows CI. ( #2075 ) use conda for installation black and isort tools ( #2068 ) CI: Fix CI builds related to new pandas 1.0 compatibility ( #2061 ) Fix data map for int8 on OmniSciDB backend ( #2056 ) Add possibility to run tests for separate backend via make test BACKENDS=[YOUR BACKEND] ( #2052 ) Fix \"cudf\" import on OmniSciDB backend ( #2055 ) CI: Drop table only if it exists (OmniSciDB) ( #2050 ) Add initial documentation for OmniSciDB, MySQL, PySpark and SparkSQL backends, add initial documentation for geospatial methods and add links to Ibis wiki page ( #2034 ) Implement covariance for bigquery backend ( #2044 ) Add Spark to supported backends list ( #2046 ) Ping dependency of rtree to fix CI failure ( #2043 ) Drop support for Python 3.5 ( #2037 ) HTML escape column names and types in png repr. ( #2023 ) Add geospatial tutorial notebook ( #1991 ) Change omniscidb image tag from v4.7.0 to v5.0.0 on docker-compose recipe ( #2031 ) Pin \"semantic_version\" to \"<2.7\" in the docs build CI, fix \"builddoc\" and \"doc\" section inside \"Makefile\" and skip mysql tzinfo on CI to allow to run MySQL using docker container on a hard disk drive. ( #2030 ) Fixed impala start up issues ( #2012 ) cache all ops in translate() ( #1999 ) Add black step to CI ( #1988 ) Json UUID any ( #1962 ) Add log for database services ( #1982 ) Fix BigQuery backend fixture so batting and awards_players fixture re\u2026 ( #1972 ) Disable BigQuery explicitly in all/test_join.py ( #1971 ) Re-formatting all files using pre-commit hook ( #1963 ) Disable codecov report upload during CI builds ( #1961 ) Developer doc enhancements ( #1960 ) Missing geospatial ops for OmniSciDB ( #1958 ) Remove pandas deprecation warnings ( #1950 ) Add developer docs to get docker setup ( #1948 ) More informative IntegrityError on duplicate columns ( #1949 ) Improve geospatial literals and smoke tests ( #1928 ) PostGIS enhancements ( #1925 ) Rename mapd to omniscidb backend ( #1866 ) Fix failing BigQuery tests ( #1926 ) Added missing null literal op ( #1917 ) Update link to Presto website ( #1895 ) Removing linting from windows ( #1896 ) Fix link to NUMFOCUS CoC ( #1884 ) Added CoC section ( #1882 ) Remove pandas exception for rows_with_max_lookback ( #1859 ) Move CI pipelines to Azure ( #1856 )","title":"Support"},{"location":"release_notes/#120-2019-06-24","text":"","title":"1.2.0 (2019-06-24)"},{"location":"release_notes/#features_4","text":"Add new geospatial functions to OmniSciDB backend ( #1836 ) allow pandas timedelta in rows_with_max_lookback ( #1838 ) Accept rows-with-max-lookback as preceding parameter ( #1825 ) PostGIS support ( #1787 )","title":"Features"},{"location":"release_notes/#bugs_3","text":"Fix call to psql causing failing CI ( #1855 ) Fix nested array literal repr ( #1851 ) Fix repr of empty schema ( #1850 ) Add max_lookback to window replace and combine functions ( #1843 ) Partially revert #1758 ( #1837 )","title":"Bugs"},{"location":"release_notes/#support_3","text":"Skip SQLAlchemy backend tests in connect method in backends.py ( #1847 ) Validate order_by when using rows_with_max_lookback window ( #1848 ) Generate release notes from commits ( #1845 ) Raise exception on backends where rows_with_max_lookback can't be implemented ( #1844 ) Tighter version spec for pytest ( #1840 ) Allow passing a branch to ci/feedstock.py ( #1826 )","title":"Support"},{"location":"release_notes/#110-2019-06-09","text":"","title":"1.1.0 (2019-06-09)"},{"location":"release_notes/#features_5","text":"Conslidate trailing window functions ( #1809 ) Call to_interval when casting integers to intervals ( #1766 ) Add session feature to mapd client API ( #1796 ) Add min periods parameter to Window ( #1792 ) Allow strings for types in pandas UDFs ( #1785 ) Add missing date operations and struct field operation for the pandas backend ( #1790 ) Add window operations to the OmniSci backend ( #1771 ) Reimplement the pandas backend using topological sort ( #1758 ) Add marker for xfailing specific backends ( #1778 ) Enable window function tests where possible ( #1777 ) is_computable_arg dispatcher ( #1743 ) Added float32 and geospatial types for create table from schema ( #1753 )","title":"Features"},{"location":"release_notes/#bugs_4","text":"Fix group_concat test and implementations ( #1819 ) Fix failing strftime tests on Python 3.7 ( #1818 ) Remove unnecessary (and erroneous in some cases) frame clauses ( #1757 ) Chained mutate operations are buggy ( #1799 ) Allow projections from joins to attempt fusion ( #1783 ) Fix Python 3.5 dependency versions ( #1798 ) Fix compatibility and bugs associated with pandas toposort reimplementation ( #1789 ) Fix outer_join generating LEFT join instead of FULL OUTER ( #1772 ) NullIf should enforce that its arguments are castable to a common type ( #1782 ) Fix conda create command in documentation ( #1775 ) Fix preceding and following with None ( #1765 ) PostgreSQL interval type not recognized ( #1661 )","title":"Bugs"},{"location":"release_notes/#support_4","text":"Remove decorator hacks and add custom markers ( #1820 ) Add development deps to setup.py ( #1814 ) Fix design and developer docs ( #1805 ) Pin sphinx version to 2.0.1 ( #1810 ) Add pep8speaks integration ( #1793 ) Fix typo in UDF signature specification ( #1821 ) Clean up most xpassing tests ( #1779 ) Update omnisci container version ( #1781 ) Constrain PyMapD version to get passing builds ( #1776 ) Remove warnings and clean up some docstrings ( #1763 ) Add StringToTimestamp as unsupported ( #1638 ) Add isort pre-commit hooks ( #1759 ) Add Python 3.5 testing back to CI ( #1750 ) Re-enable CI for building step ( #1700 ) Update README reference to MapD to say OmniSci ( #1749 )","title":"Support"},{"location":"release_notes/#100-2019-03-26","text":"","title":"1.0.0 (2019-03-26)"},{"location":"release_notes/#features_6","text":"Add black as a pre-commit hook ( #1735 ) Add support for the arbitrary aggregate in the mapd backend ( #1680 ) Add SQL method for the MapD backend ( #1731 ) Clean up merge PR script and use the actual merge feature of GitHub ( #1744 ) Add cross join to the pandas backend ( #1723 ) Implement default handler for multiple client pre_execute ( #1727 ) Implement BigQuery auth using pydata_google_auth ( #1728 ) Timestamp literal accepts a timezone parameter ( #1712 ) Remove support for passing integers to ibis.timestamp ( #1725 ) Add find_nodes to lineage ( #1704 ) Remove a bunch of deprecated APIs and clean up warnings ( #1714 ) Implement table distinct for the pandas backend ( #1716 ) Implement geospatial functions for MapD ( #1678 ) Implement geospatial types for MapD ( #1666 ) Add pre commit hook ( #1685 ) Getting started with mapd, mysql and pandas ( #1686 ) Support column names with special characters in mapd ( #1675 ) Allow operations to hide arguments from display ( #1669 ) Remove implicit ordering requirements in the PostgreSQL backend ( #1636 ) Add cross join operator to MapD ( #1655 ) Fix UDF bugs and add support for non-aggregate analytic functions ( #1637 ) Support string slicing with other expressions ( #1627 ) Publish the ibis roadmap ( #1618 ) Implement approx_median in BigQuery ( #1604 ) Make ibis node instances hashable ( #1611 ) Add range_window and trailing_range_window to docs ( #1608 )","title":"Features"},{"location":"release_notes/#bugs_5","text":"Make dev/merge-pr.py script handle PR branches ( #1745 ) Fix NULLIF implementation for the pandas backend ( #1742 ) Fix casting to float in the MapD backend ( #1737 ) Fix testing for BigQuery after auth flow update ( #1741 ) Fix skipping for new BigQuery auth flow ( #1738 ) Fix bug in TableExpr.drop ( #1732 ) Filter the raw warning from newer pandas to support older pandas ( #1729 ) Fix BigQuery credentials link ( #1706 ) Add Union as an unsuppoted operation for MapD ( #1639 ) Fix visualizing an ibis expression when showing a selection after a table join ( #1705 ) Fix MapD exception for toDateTime ( #1659 ) Use == to compare strings ( #1701 ) Resolves joining with different column names ( #1647 ) Fix map get with compatible types ( #1643 ) Fixed where operator for MapD ( #1653 ) Remove parameters from mapd ( #1648 ) Make sure we cast when NULL is else in CASE expressions ( #1651 ) Fix equality ( #1600 )","title":"Bugs"},{"location":"release_notes/#support_5","text":"Do not build universal wheels ( #1748 ) Remove tag prefix from versioneer ( #1747 ) Use releases to manage documentation ( #1746 ) Use cudf instead of pygdf ( #1694 ) Fix multiple CI issues ( #1696 ) Update mapd ci to v4.4.1 ( #1681 ) Enabled mysql CI on azure pipelines ( #1672 ) Remove support for Python 2 ( #1670 ) Fix flake8 and many other warnings ( #1667 ) Update README.md for impala and kudu ( #1664 ) Remove defaults as a channel from azure pipelines ( #1660 ) Fixes a very typo in the pandas/core.py docstring ( #1658 ) Unpin clickhouse-driver version ( #1657 ) Add test for reduction returning lists ( #1650 ) Fix Azure VM image name ( #1646 ) Updated MapD server-CI ( #1641 ) Add TableExpr.drop to API documentation ( #1645 ) Fix Azure deployment step ( #1642 ) Set up CI with Azure Pipelines ( #1640 ) Fix conda builds ( #1609 )","title":"Support"},{"location":"release_notes/#v0140-2018-08-23","text":"This release brings refactored, more composable core components and rule system to ibis. We also focused quite heavily on the BigQuery backend this release.","title":"v0.14.0 (2018-08-23)"},{"location":"release_notes/#new-features","text":"Allow keyword arguments in Node subclasses ( #968 ) Splat args into Node subclasses instead of requiring a list ( #969 ) Add support for UNION in the BigQuery backend ( #1408 , #1409 ) Support for writing UDFs in BigQuery ( #1377 ). See the BigQuery UDF docs for more details. Support for cross-project expressions in the BigQuery backend. ( #1427 , #1428 ) Add strftime and to_timestamp support for BigQuery ( #1422 , #1410 ) Require google-cloud-bigquery >=1.0 ( #1424 ) Limited support for interval arithmetic in the pandas backend ( #1407 ) Support for subclassing TableExpr ( #1439 ) Fill out pandas backend operations ( #1423 ) Add common DDL APIs to the pandas backend ( #1464 ) Implement the sql method for BigQuery ( #1463 ) Add to_timestamp for BigQuery ( #1455 ) Add the mapd backend ( #1419 ) Implement range windows ( #1349 ) Support for map types in the pandas backend ( #1498 ) Add mean and sum for boolean types in BigQuery ( #1516 ) All recent versions of SQLAlchemy are now suppported ( #1384 ) Add support for NUMERIC types in the BigQuery backend ( #1534 ) Speed up grouped and rolling operations in the pandas backend ( #1549 ) Implement TimestampNow for BigQuery and pandas ( #1575 )","title":"New Features"},{"location":"release_notes/#bug-fixes_2","text":"Nullable property is now propagated through value types ( #1289 ) Implicit casting between signed and unsigned integers checks boundaries Fix precedence of case statement ( #1412 ) Fix handling of large timestamps ( #1440 ) Fix identical_to precedence ( #1458 ) Pandas 0.23 compatibility ( #1458 ) Preserve timezones in timestamp-typed literals ( #1459 ) Fix incorrect topological ordering of UNION expressions ( #1501 ) Fix projection fusion bug when attempting to fuse columns of the same name ( #1496 ) Fix output type for some decimal operations ( #1541 )","title":"Bug Fixes"},{"location":"release_notes/#api-changes","text":"The previous, private rules API has been rewritten ( #1366 ) Defining input arguments for operations happens in a more readable fashion instead of the previous [input_type]{.title-ref} list. Removed support for async query execution (only Impala supported) Remove support for Python 3.4 ( #1326 ) BigQuery division defaults to using IEEE_DIVIDE ( #1390 ) Add tolerance parameter to asof_join ( #1443 )","title":"API Changes"},{"location":"release_notes/#v0130-2018-03-30","text":"This release brings new backends, including support for executing against files, MySQL, Pandas user defined scalar and aggregations along with a number of bug fixes and reliability enhancements. We recommend that all users upgrade from earlier versions of Ibis.","title":"v0.13.0 (2018-03-30)"},{"location":"release_notes/#new-backends","text":"File Support for CSV & HDF5 ( #1165 , #1194 ) File Support for Parquet Format ( #1175 , #1194 ) Experimental support for MySQL thanks to \\@kszucs ( #1224 )","title":"New Backends"},{"location":"release_notes/#new-features_1","text":"Support for Unsigned Integer Types ( #1194 ) Support for Interval types and expressions with support for execution on the Impala and Clickhouse backends ( #1243 ) Isnan, isinf operations for float and double values ( #1261 ) Support for an interval with a quarter period ( #1259 ) ibis.pandas.from_dataframe convenience function ( #1155 ) Remove the restriction on ROW_NUMBER() requiring it to have an ORDER BY clause ( #1371 ) Add .get() operation on a Map type ( #1376 ) Allow visualization of custom defined expressions Add experimental support for pandas UDFs/UDAFs ( #1277 ) Functions can be used as groupby keys ( #1214 , #1215 ) Generalize the use of the where parameter to reduction operations ( #1220 ) Support for interval operations thanks to \\@kszucs ( #1243 , #1260 , #1249 ) Support for the PARTITIONTIME column in the BigQuery backend ( #1322 ) Add arbitrary() method for selecting the first non null value in a column ( #1230 , #1309 ) Windowed MultiQuantile operation in the pandas backend thanks to \\@DiegoAlbertoTorres ( #1343 ) Rules for validating table expressions thanks to \\@DiegoAlbertoTorres ( #1298 ) Complete end-to-end testing framework for all supported backends ( #1256 ) contains / not contains now supported in the pandas backend ( #1210 , #1211 ) CI builds are now reproducible locally thanks to \\@kszucs ( #1121 , #1237 , #1255 , #1311 ) isnan / isinf operations thanks to \\@kszucs ( #1261 ) Framework for generalized dtype and schema inference, and implicit casting thanks to \\@kszucs ( #1221 , #1269 ) Generic utilities for expression traversal thanks to \\@kszucs ( #1336 ) day_of_week API ( #306 , #1047 ) Design documentation for ibis ( #1351 )","title":"New Features"},{"location":"release_notes/#bug-fixes_3","text":"Unbound parameters were failing in the simple case of a ibis.expr.types.TableExpr.mutate call with no operation ( #1378 ) Fix parameterized subqueries ( #1300 , #1331 , #1303 , #1378 ) Fix subquery extraction, which wasn\\'t happening in topological order ( #1342 ) Fix parenthesization if isnull ( #1307 ) Calling drop after mutate did not work ( #1296 , #1299 ) SQLAlchemy backends were missing an implementation of ibis.expr.operations.NotContains . Support REGEX_EXTRACT in PostgreSQL 10 ( #1276 , #1278 )","title":"Bug Fixes"},{"location":"release_notes/#api-changes_1","text":"Fixing #1378 required the removal of the name parameter to the ibis.param function. Use the ibis.expr.types.Expr.name method instead.","title":"API Changes"},{"location":"release_notes/#v0120-2017-10-28","text":"This release brings Clickhouse and BigQuery SQL support along with a number of bug fixes and reliability enhancements. We recommend that all users upgrade from earlier versions of Ibis.","title":"v0.12.0 (2017-10-28)"},{"location":"release_notes/#new-backends_1","text":"BigQuery backend ( #1170 ), thanks to \\@tsdlovell. Clickhouse backend ( #1127 ), thanks to \\@kszucs.","title":"New Backends"},{"location":"release_notes/#new-features_2","text":"Add support for Binary data type ( #1183 ) Allow users of the BigQuery client to define their own API proxy classes ( #1188 ) Add support for HAVING in the pandas backend ( #1182 ) Add struct field tab completion ( #1178 ) Add expressions for Map/Struct types and columns ( #1166 ) Support Table.asof_join ( #1162 ) Allow right side of arithmetic operations to take over ( #1150 ) Add a data_preload step in pandas backend ( #1142 ) expressions in join predicates in the pandas backend ( #1138 ) Scalar parameters ( #1075 ) Limited window function support for pandas ( #1083 ) Implement Time datatype ( #1105 ) Implement array ops for pandas ( #1100 ) support for passing multiple quantiles in .quantile() ( #1094 ) support for clip and quantile ops on DoubleColumns ( #1090 ) Enable unary math operations for pandas, sqlite ( #1071 ) Enable casting from strings to temporal types ( #1076 ) Allow selection of whole tables in pandas joins ( #1072 ) Implement comparison for string vs date and timestamp types ( #1065 ) Implement isnull and notnull for pandas ( #1066 ) Allow like operation to accept a list of conditions to match ( #1061 ) Add a pre_execute step in pandas backend ( #1189 )","title":"New Features"},{"location":"release_notes/#bug-fixes_4","text":"Remove global expression caching to ensure repeatable code generation ( #1179 , #1181 ) Fix ORDER BY generation without a GROUP BY ( #1180 , #1181 ) Ensure that ~ibis.expr.datatypes.DataType and subclasses hash properly ( #1172 ) Ensure that the pandas backend can deal with unary operations in groupby ( #1182 ) Incorrect impala code generated for NOT with complex argument ( #1176 ) BUG/CLN: Fix predicates on Selections on Joins ( #1149 ) Don\\'t use SET LOCAL to allow redshift to work ( #1163 ) Allow empty arrays as arguments ( #1154 ) Fix column renaming in groupby keys ( #1151 ) Ensure that we only cast if timezone is not None ( #1147 ) Fix location of conftest.py ( #1107 ) TST/Make sure we drop tables during postgres testing ( #1101 ) Fix misleading join error message ( #1086 ) BUG/TST: Make hdfs an optional dependency ( #1082 ) Memoization should include expression name where available ( #1080 )","title":"Bug Fixes"},{"location":"release_notes/#performance-enhancements","text":"Speed up imports ( #1074 ) Fix execution perf of groupby and selection ( #1073 ) Use normalize for casting to dates in pandas ( #1070 ) Speed up pandas groupby ( #1067 )","title":"Performance Enhancements"},{"location":"release_notes/#contributors","text":"The following people contributed to the 0.12.0 release : $ git shortlog -sn --no-merges v0.11.2..v0.12.0 63 Phillip Cloud 8 Jeff Reback 2 Kriszti\u00e1n Sz\u0171cs 2 Tory Haavik 1 Anirudh 1 Szucs Krisztian 1 dlovell 1 kwangin","title":"Contributors"},{"location":"release_notes/#0110-2017-06-28","text":"This release brings initial Pandas backend support along with a number of bug fixes and reliability enhancements. We recommend that all users upgrade from earlier versions of Ibis.","title":"0.11.0 (2017-06-28)"},{"location":"release_notes/#new-features_3","text":"Experimental pandas backend to allow execution of ibis expression against pandas DataFrames Graphviz visualization of ibis expressions. Implements _repr_png_ for Jupyter Notebook functionality Ability to create a partitioned table from an ibis expression Support for missing operations in the SQLite backend: sqrt, power, variance, and standard deviation, regular expression functions, and missing power support for PostgreSQL Support for schemas inside databases with the PostgreSQL backend Appveyor testing on core ibis across all supported Python versions Add year / month / day methods to date types Ability to sort, group by and project columns according to positional index rather than only by name Added a type parameter to ibis.literal to allow user specification of literal types","title":"New Features"},{"location":"release_notes/#bug-fixes_5","text":"Fix broken conda recipe Fix incorrectly typed fillna operation Fix postgres boolean summary operations Fix kudu support to reflect client API Changes Fix equality of nested types and construction of nested types when the value type is specified as a string","title":"Bug Fixes"},{"location":"release_notes/#api-changes_2","text":"Deprecate passing integer values to the ibis.timestamp literal constructor, this will be removed in 0.12.0 Added the admin_timeout parameter to the kudu client connect function","title":"API Changes"},{"location":"release_notes/#contributors_1","text":"$ git shortlog --summary --numbered v0.10.0..v0.11.0 58 Phillip Cloud 1 Greg Rahn 1 Marius van Niekerk 1 Tarun Gogineni 1 Wes McKinney","title":"Contributors"},{"location":"release_notes/#08-2016-05-19","text":"This release brings initial PostgreSQL backend support along with a number of critical bug fixes and usability improvements. As several correctness bugs with the SQL compiler were fixed, we recommend that all users upgrade from earlier versions of Ibis.","title":"0.8 (2016-05-19)"},{"location":"release_notes/#new-features_4","text":"Initial PostgreSQL backend contributed by Phillip Cloud. Add groupby as an alias for group_by to table expressions","title":"New Features"},{"location":"release_notes/#bug-fixes_6","text":"Fix an expression error when filtering based on a new field Fix Impala\\'s SQL compilation of using OR with compound filters Various fixes with the having(...) function in grouped table expressions Fix CTE ( WITH ) extraction inside UNION ALL expressions. Fix ImportError on Python 2 when mock library not installed","title":"Bug Fixes"},{"location":"release_notes/#api-changes_3","text":"The deprecated ibis.impala_connect and ibis.make_client APIs have been removed","title":"API Changes"},{"location":"release_notes/#07-2016-03-16","text":"This release brings initial Kudu-Impala integration and improved Impala and SQLite support, along with several critical bug fixes.","title":"0.7 (2016-03-16)"},{"location":"release_notes/#new-features_5","text":"Apache Kudu (incubating) integration for Impala users. See the blog post for now. Will add some documentation here when possible. Add use_https option to ibis.hdfs_connect for WebHDFS connections in secure (Kerberized) clusters without SSL enabled. Correctly compile aggregate expressions involving multiple subqueries. To explain this last point in more detail, suppose you had: table = ibis . table ([( 'flag' , 'string' ), ( 'value' , 'double' )], 'tbl' ) flagged = table [ table . flag == '1' ] unflagged = table [ table . flag == '0' ] fv = flagged . value uv = unflagged . value expr = ( fv . mean () / fv . sum ()) - ( uv . mean () / uv . sum ()) The last expression now generates the correct Impala or SQLite SQL: SELECT t0 . ` tmp ` - t1 . ` tmp ` AS ` tmp ` FROM ( SELECT avg ( ` value ` ) / sum ( ` value ` ) AS ` tmp ` FROM tbl WHERE ` flag ` = '1' ) t0 CROSS JOIN ( SELECT avg ( ` value ` ) / sum ( ` value ` ) AS ` tmp ` FROM tbl WHERE ` flag ` = '0' ) t1","title":"New Features"},{"location":"release_notes/#bug-fixes_7","text":"CHAR(n) and VARCHAR(n) Impala types now correctly map to Ibis string expressions Fix inappropriate projection-join-filter expression rewrites resulting in incorrect generated SQL. ImpalaClient.create_table correctly passes STORED AS PARQUET for format='parquet' . Fixed several issues with Ibis dependencies (impyla, thriftpy, sasl, thrift_sasl), especially for secure clusters. Upgrading will pull in these new dependencies. Do not fail in ibis.impala.connect when trying to create the temporary Ibis database if no HDFS connection passed. Fix join predicate evaluation bug when column names overlap with table attributes. Fix handling of fully-materialized joins (aka select * joins) in SQLAlchemy / SQLite.","title":"Bug Fixes"},{"location":"release_notes/#contributors_2","text":"Thank you to all who contributed patches to this release. $ git log v0.6.0..v0.7.0 --pretty=format:%aN | sort | uniq -c | sort -rn 21 Wes McKinney 1 Uri Laserson 1 Kristopher Overholt","title":"Contributors"},{"location":"release_notes/#06-2015-12-01","text":"This release brings expanded pandas and Impala integration, including support for managing partitioned tables in Impala. See the new Ibis for Impala Users guide for more on using Ibis with Impala. The Ibis for SQL Programmers guide also was written since the 0.5 release. This release also includes bug fixes affecting generated SQL correctness. All users should upgrade as soon as possible.","title":"0.6 (2015-12-01)"},{"location":"release_notes/#new-features_6","text":"New integrated Impala functionality. See Ibis for Impala Users for more details on these things. Improved Impala-pandas integration. Create tables or insert into existing tables from pandas DataFrame objects. Partitioned table metadata management API. Add, drop, alter, and insert into table partitions. Add is_partitioned property to ImpalaTable . Added support for LOAD DATA DDL using the load_data function, also supporting partitioned tables. Modify table metadata (location, format, SerDe properties etc.) using ImpalaTable.alter Interrupting Impala expression execution with Control-C will attempt to cancel the running query with the server. Set the compression codec (e.g. snappy) used with ImpalaClient.set_compression_codec . Get and set query options for a client session with ImpalaClient.get_options and ImpalaClient.set_options . Add ImpalaTable.metadata method that parses the output of the DESCRIBE FORMATTED DDL to simplify table metadata inspection. Add ImpalaTable.stats and ImpalaTable.column_stats to see computed table and partition statistics. Add CHAR and VARCHAR handling Add refresh , invalidate_metadata DDL options and add incremental option to compute_stats for COMPUTE INCREMENTAL STATS . Add substitute method for performing multiple value substitutions in an array or scalar expression. Division is by default true division like Python 3 for all numeric data. This means for SQL systems that use C-style division semantics, the appropriate CAST will be automatically inserted in the generated SQL. Easier joins on tables with overlapping column names. See Ibis for SQL Programmers . Expressions like string_expr[:3] now work as expected. Add coalesce instance method to all value expressions. Passing limit=None to the execute method on expressions disables any default row limits.","title":"New Features"},{"location":"release_notes/#api-changes_4","text":"ImpalaTable.rename no longer mutates the calling table expression.","title":"API Changes"},{"location":"release_notes/#contributors_3","text":"$ git log v0.5.0..v0.6.0 --pretty=format:%aN | sort | uniq -c | sort -rn 46 Wes McKinney 3 Uri Laserson 1 Phillip Cloud 1 mariusvniekerk 1 Kristopher Overholt","title":"Contributors"},{"location":"release_notes/#05-2015-09-10","text":"Highlights in this release are the SQLite, Python 3, Impala UDA support, and an asynchronous execution API. There are also many usability improvements, bug fixes, and other new features.","title":"0.5 (2015-09-10)"},{"location":"release_notes/#new-features_7","text":"SQLite client and built-in function support Ibis now supports Python 3.4 as well as 2.6 and 2.7 Ibis can utilize Impala user-defined aggregate (UDA) functions SQLAlchemy-based translation toolchain to enable more SQL engines having SQLAlchemy dialects to be supported Many window function usability improvements (nested analytic functions and deferred binding conveniences) More convenient aggregation with keyword arguments in aggregate functions Built preliminary wrapper API for MADLib-on-Impala Add var and std aggregation methods and support in Impala Add nullifzero numeric method for all SQL engines Add rename method to Impala tables (for renaming tables in the Hive metastore) Add close method to ImpalaClient for session cleanup (#533) Add relabel method to table expressions Add insert method to Impala tables Add compile and verify methods to all expressions to test compilation and ability to compile (since many operations are unavailable in SQLite, for example)","title":"New Features"},{"location":"release_notes/#api-changes_5","text":"Impala Ibis client creation now uses only ibis.impala.connect , and ibis.make_client has been deprecated","title":"API Changes"},{"location":"release_notes/#contributors_4","text":"$ git log v0.4.0..v0.5.0 --pretty=format:%aN | sort | uniq -c | sort -rn 55 Wes McKinney 9 Uri Laserson 1 Kristopher Overholt","title":"Contributors"},{"location":"release_notes/#04-2015-08-14","text":"","title":"0.4 (2015-08-14)"},{"location":"release_notes/#new-features_8","text":"Add tooling to use Impala C++ scalar UDFs within Ibis (#262, #195) Support and testing for Kerberos-enabled secure HDFS clusters Many table functions can now accept functions as parameters (invoked on the calling table) to enhance composability and emulate late-binding semantics of languages (like R) that have non-standard evaluation (#460) Add any , all , notany , and notall reductions on boolean arrays, as well as cumany and cumall Using topk now produces an analytic expression that is executable (as an aggregation) but can also be used as a filter as before (#392, #91) Added experimental database object \\\"usability layer\\\", see ImpalaClient.database . Add TableExpr.info Add compute_stats API to table expressions referencing physical Impala tables Add explain method to ImpalaClient to show query plan for an expression Add chmod and chown APIs to HDFS interface for superusers Add convert_base method to strings and integer types Add option to ImpalaClient.create_table to create empty partitioned tables ibis.cross_join can now join more than 2 tables at once Add ImpalaClient.raw_sql method for running naked SQL queries ImpalaClient.insert now validates schemas locally prior to sending query to cluster, for better usability. Add conda installation recipes","title":"New Features"},{"location":"release_notes/#contributors_5","text":"$ git log v0.3.0..v0.4.0 --pretty=format:%aN | sort | uniq -c | sort -rn 38 Wes McKinney 9 Uri Laserson 2 Meghana Vuyyuru 2 Kristopher Overholt 1 Marius van Niekerk","title":"Contributors"},{"location":"release_notes/#03-2015-07-20","text":"First public release. See http://ibis-project.org for more.","title":"0.3 (2015-07-20)"},{"location":"release_notes/#new-features_9","text":"Implement window / analytic function support Enable non-equijoins (join clauses with operations other than == ). Add remaining string functions supported by Impala. Add pipe method to tables (hat-tip to the pandas dev team). Add mutate convenience method to tables. Fleshed out WebHDFS implementations: get/put directories, move files, etc. See the full HDFS API . Add truncate method for timestamp values ImpalaClient can execute scalar expressions not involving any table. Can also create internal Impala tables with a specific HDFS path. Make Ibis\\'s temporary Impala database and HDFS paths configurable (see ibis.options ). Add truncate_table function to client (if the user\\'s Impala cluster supports it). Python 2.6 compatibility Enable Ibis to execute concurrent queries in multithreaded applications (earlier versions were not thread-safe). Test data load script in scripts/load_test_data.py Add an internal operation type signature API to enhance developer productivity.","title":"New Features"},{"location":"release_notes/#contributors_6","text":"$ git log v0.2.0..v0.3.0 --pretty=format:%aN | sort | uniq -c | sort -rn 59 Wes McKinney 29 Uri Laserson 4 Isaac Hodes 2 Meghana Vuyyuru","title":"Contributors"},{"location":"release_notes/#02-2015-06-16","text":"","title":"0.2 (2015-06-16)"},{"location":"release_notes/#new-features_10","text":"insert method on Ibis client for inserting data into existing tables. parquet_file , delimited_file , and avro_file client methods for querying datasets not yet available in Impala New ibis.hdfs_connect method and HDFS client API for WebHDFS for writing files and directories to HDFS New timedelta API and improved timestamp data support New bucket and histogram methods on numeric expressions New category logical datatype for handling bucketed data, among other things Add summary API to numeric expressions Add value_counts convenience API to array expressions New string methods like , rlike , and contains for fuzzy and regex searching Add options.verbose option and configurable options.verbose_log callback function for improved query logging and visibility Support for new SQL built-in functions ibis.coalesce ibis.greatest and ibis.least ibis.where for conditional logic (see also ibis.case and ibis.cases ) nullif method on value expressions ibis.now New aggregate functions: approx_median , approx_nunique , and group_concat where argument in aggregate functions Add having method to group_by intermediate object Added group-by convenience table.group_by(exprs).COLUMN_NAME.agg_function() Add default expression names to most aggregate functions New Impala database client helper methods create_database drop_database exists_database list_databases set_database Client list_tables searching / listing method Add add , sub , and other explicit arithmetic methods to value expressions","title":"New Features"},{"location":"release_notes/#api-changes_6","text":"New Ibis client and Impala connection workflow. Client now combined from an Impala connection and an optional HDFS connection","title":"API Changes"},{"location":"release_notes/#bug-fixes_8","text":"Numerous expression API bug fixes and rough edges fixed","title":"Bug Fixes"},{"location":"release_notes/#contributors_7","text":"$ git log v0.1.0..v0.2.0 --pretty=format:%aN | sort | uniq -c | sort -rn 71 Wes McKinney 1 Juliet Hougland 1 Isaac Hodes","title":"Contributors"},{"location":"release_notes/#01-2015-03-26","text":"First Ibis release. Expression DSL design and type system Expression to ImpalaSQL compiler toolchain Impala built-in function wrappers $ git log 84d0435..v0.1.0 --pretty=format:%aN | sort | uniq -c | sort -rn 78 Wes McKinney 1 srus 1 Henry Robinson","title":"0.1 (2015-03-26)"},{"location":"about/","text":"Ibis: Python Data Analysis Productivity Framework Ibis is a toolbox to bridge the gap between local Python environments (like pandas and scikit-learn) and remote storage and execution systems like Hadoop components (like HDFS, Impala, Hive, Spark) and SQL databases (Postgres, etc.). Its goal is to simplify analytical workflows and make you more productive. We have a handful of specific priority focus areas: Enable data analysts to translate local, single-node data idioms to scalable computation representations (e.g. SQL or Spark) Integration with pandas and other Python data ecosystem components Provide high level analytics APIs and workflow tools to enhance productivity and streamline common or tedious tasks. Integration with community standard data formats (e.g. Parquet and Avro) Abstract away database-specific SQL differences As the Apache Arrow project develops, we will look to use Arrow to enable computational code written in Python to be executed natively within other systems like Apache Spark and Apache Impala (incubating). Source code is on GitHub: https://github.com/ibis-project/ibis .","title":"Introduction"},{"location":"about/#ibis-python-data-analysis-productivity-framework","text":"Ibis is a toolbox to bridge the gap between local Python environments (like pandas and scikit-learn) and remote storage and execution systems like Hadoop components (like HDFS, Impala, Hive, Spark) and SQL databases (Postgres, etc.). Its goal is to simplify analytical workflows and make you more productive. We have a handful of specific priority focus areas: Enable data analysts to translate local, single-node data idioms to scalable computation representations (e.g. SQL or Spark) Integration with pandas and other Python data ecosystem components Provide high level analytics APIs and workflow tools to enhance productivity and streamline common or tedious tasks. Integration with community standard data formats (e.g. Parquet and Avro) Abstract away database-specific SQL differences As the Apache Arrow project develops, we will look to use Arrow to enable computational code written in Python to be executed natively within other systems like Apache Spark and Apache Impala (incubating). Source code is on GitHub: https://github.com/ibis-project/ibis .","title":"Ibis: Python Data Analysis Productivity Framework"},{"location":"about/license/","text":"Legal Ibis is distributed under the Apache License, Version 2.0. Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION 1. Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. 4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. 6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright [yyyy] [name of copyright owner] Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"about/license/#legal","text":"Ibis is distributed under the Apache License, Version 2.0. Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION 1. Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. 2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. 3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. 4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. 5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. 6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. 7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. 8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. 9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright [yyyy] [name of copyright owner] Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Legal"},{"location":"about/roadmap/","text":"Roadmap This document is an outline of the next set of major efforts within ibis. Long Term Goals This section outlines broader, longer-term goals for the project alongside a few short-term goals and provides information and direction for a few key areas of focus over the next 1-2 years, possibly longer depending on the amount of time the developers of Ibis can devote to the project. Compiler Structure Separation of Concerns The current architecture of the ibis compiler has a few key problems that need to be addressed to ensure longevity and maintainability of the project going forward. The main structural problem is that there isn\u2019t one place where expression optimizations and transformations happen. Sometimes optimizations occur as an expression is being built, and other times they occur on a whole expression. The solution is to separate the expression construction and optimization into two phases, and guarantee that a constructed expression, if compiled without optimization, maps clearly to SQL constructs. The optimization pass would happen just before compilation and would be free to optimize whole expression trees. This approach lets us optimize queries piece by piece, as opposed to having to provide all optimization implementations in a single pull request. Unifying Table and Column Compilation Right now, it is very difficult to customize the way the operations underlying table expressions are compiled. The logic to compile them is hard-coded in each backend (or the compiler\u2019s parent class). This needs to be addressed, if only to ease the burden of implementing the UNNEST operation and make the codebase easier to understand and maintain. Depth \"Depth\" goals relate to enhancing Ibis to provide better support for backend-specific functionality. Backend-Specific Operations As the number of ibis users and use cases grows there will be an increasing need for individual backends to support more exotic operations. Many SQL databases have features that are unique only to themselves and often this is why people will choose that technology over another. Ibis should support an API that reflects the backend that underlies an expression and expose the functionality of that specific backend. A concrete example of this is the FARM_FINGERPRINT function in BigQuery. It is unlikely that the main ValueExpr API will ever grow such a method, but a BigQuery user shouldn\u2019t be restricted to using only the methods this API provides. Moreover, users should be able to bring their own methods to this API without having to consult the ibis developers and without the addition of such operations polluting the namespace of the main API. One drawback to enabling this is that it provides an incentive for people to define operations with a backend-specific spelling (presumably in the name of expediency) that may actually be easily generalizable to or useful for other backends. This behavior should be discouraged to the extent possible. Standardize UDFs (User Defined Functions) A few backends have support for UDFs. Impala, Pandas and BigQuery all have at least some level of support for user-defined functions. This mechanism should be extended to other backends where possible. We outline different approaches to adding UDFs to the backends that are well-supported but currently do not have a UDF implementation. Development of a standard interface for UDFs is ideal, so that it\u2019s easy for new backends to implement the interface. Breadth The major breadth-related question ibis is facing is how to grow the number of backends in ibis in a scalable, minimum-maintenance way is an open question. Currently there is a test suite that runs across all backends that xfails tests when a backend doesn\u2019t implement a particular operation. At minimum we need a way to display which backends implement which operations. With the ability to provide custom operations we also need a way to display the custom operations that each backend provides. Backend-Specific Goals These goals relate to specific backends pandas Speed up grouped, rolling, and simple aggregations using numba pandas aggregations are quite slow relative to an equivalent numba implementation, for various reasons. Since ibis hides the implementation details of a particular expression we can experiment with using different aggregation implementations. Dask Implement a Dask backend There is currently no way in ibis to easily parallelize a computation on a single machine, let alone distribute a computation across machines. Dask provides APIs for doing such things. Spark Implement a SparkSQL backend SparkSQL provides a way to execute distributed SQL queries similar to other backends supported by ibis such as Impala and BigQuery.","title":"Roadmap"},{"location":"about/roadmap/#roadmap","text":"This document is an outline of the next set of major efforts within ibis.","title":"Roadmap"},{"location":"about/roadmap/#long-term-goals","text":"This section outlines broader, longer-term goals for the project alongside a few short-term goals and provides information and direction for a few key areas of focus over the next 1-2 years, possibly longer depending on the amount of time the developers of Ibis can devote to the project.","title":"Long Term Goals"},{"location":"about/roadmap/#compiler-structure","text":"","title":"Compiler Structure"},{"location":"about/roadmap/#separation-of-concerns","text":"The current architecture of the ibis compiler has a few key problems that need to be addressed to ensure longevity and maintainability of the project going forward. The main structural problem is that there isn\u2019t one place where expression optimizations and transformations happen. Sometimes optimizations occur as an expression is being built, and other times they occur on a whole expression. The solution is to separate the expression construction and optimization into two phases, and guarantee that a constructed expression, if compiled without optimization, maps clearly to SQL constructs. The optimization pass would happen just before compilation and would be free to optimize whole expression trees. This approach lets us optimize queries piece by piece, as opposed to having to provide all optimization implementations in a single pull request.","title":"Separation of Concerns"},{"location":"about/roadmap/#unifying-table-and-column-compilation","text":"Right now, it is very difficult to customize the way the operations underlying table expressions are compiled. The logic to compile them is hard-coded in each backend (or the compiler\u2019s parent class). This needs to be addressed, if only to ease the burden of implementing the UNNEST operation and make the codebase easier to understand and maintain.","title":"Unifying Table and Column Compilation"},{"location":"about/roadmap/#depth","text":"\"Depth\" goals relate to enhancing Ibis to provide better support for backend-specific functionality.","title":"Depth"},{"location":"about/roadmap/#backend-specific-operations","text":"As the number of ibis users and use cases grows there will be an increasing need for individual backends to support more exotic operations. Many SQL databases have features that are unique only to themselves and often this is why people will choose that technology over another. Ibis should support an API that reflects the backend that underlies an expression and expose the functionality of that specific backend. A concrete example of this is the FARM_FINGERPRINT function in BigQuery. It is unlikely that the main ValueExpr API will ever grow such a method, but a BigQuery user shouldn\u2019t be restricted to using only the methods this API provides. Moreover, users should be able to bring their own methods to this API without having to consult the ibis developers and without the addition of such operations polluting the namespace of the main API. One drawback to enabling this is that it provides an incentive for people to define operations with a backend-specific spelling (presumably in the name of expediency) that may actually be easily generalizable to or useful for other backends. This behavior should be discouraged to the extent possible.","title":"Backend-Specific Operations"},{"location":"about/roadmap/#standardize-udfs-user-defined-functions","text":"A few backends have support for UDFs. Impala, Pandas and BigQuery all have at least some level of support for user-defined functions. This mechanism should be extended to other backends where possible. We outline different approaches to adding UDFs to the backends that are well-supported but currently do not have a UDF implementation. Development of a standard interface for UDFs is ideal, so that it\u2019s easy for new backends to implement the interface.","title":"Standardize UDFs (User Defined Functions)"},{"location":"about/roadmap/#breadth","text":"The major breadth-related question ibis is facing is how to grow the number of backends in ibis in a scalable, minimum-maintenance way is an open question. Currently there is a test suite that runs across all backends that xfails tests when a backend doesn\u2019t implement a particular operation. At minimum we need a way to display which backends implement which operations. With the ability to provide custom operations we also need a way to display the custom operations that each backend provides.","title":"Breadth"},{"location":"about/roadmap/#backend-specific-goals","text":"These goals relate to specific backends","title":"Backend-Specific Goals"},{"location":"about/roadmap/#pandas","text":"","title":"pandas"},{"location":"about/roadmap/#speed-up-grouped-rolling-and-simple-aggregations-using-numba","text":"pandas aggregations are quite slow relative to an equivalent numba implementation, for various reasons. Since ibis hides the implementation details of a particular expression we can experiment with using different aggregation implementations.","title":"Speed up grouped, rolling, and simple aggregations using numba"},{"location":"about/roadmap/#dask","text":"","title":"Dask"},{"location":"about/roadmap/#implement-a-dask-backend","text":"There is currently no way in ibis to easily parallelize a computation on a single machine, let alone distribute a computation across machines. Dask provides APIs for doing such things.","title":"Implement a Dask backend"},{"location":"about/roadmap/#spark","text":"Implement a SparkSQL backend SparkSQL provides a way to execute distributed SQL queries similar to other backends supported by ibis such as Impala and BigQuery.","title":"Spark"},{"location":"about/team/","text":"Team Contributors ibis is developed and maintained by a community of volunteer contributors . Active maintainers jreback datapythonista cpcloud kszucs Former maintainers wesm ibis aims to be a welcoming, friendly, diverse and inclusive community. Everybody is welcome, regardless of gender, sexual orientation, gender identity, and expression, disability, physical appearance, body size, race, or religion. We do not tolerate harassment of community members in any form. In particular, people from underrepresented groups are encouraged to join the community.","title":"Team"},{"location":"about/team/#team","text":"","title":"Team"},{"location":"about/team/#contributors","text":"ibis is developed and maintained by a community of volunteer contributors .","title":"Contributors"},{"location":"about/team/#active-maintainers","text":"jreback datapythonista cpcloud kszucs","title":"Active maintainers"},{"location":"about/team/#former-maintainers","text":"wesm ibis aims to be a welcoming, friendly, diverse and inclusive community. Everybody is welcome, regardless of gender, sexual orientation, gender identity, and expression, disability, physical appearance, body size, race, or religion. We do not tolerate harassment of community members in any form. In particular, people from underrepresented groups are encouraged to join the community.","title":"Former maintainers"},{"location":"community/coc/","text":"Code of Conduct Ibis is governed by the NumFOCUS code of conduct : Quote Be kind to others. Do not insult or put down others. Behave professionally. Remember that harassment and sexist, racist, or exclusionary jokes are not appropriate for IBIS. All communication should be appropriate for a professional audience including people of many different backgrounds. Sexual language and imagery is not appropriate. Ibis is dedicated to providing a harassment-free community for everyone, regardless of gender, sexual orientation, gender identity, and expression, disability, physical appearance, body size, race, or religion. We do not tolerate harassment of community members in any form. Thank you for helping make this a welcoming, friendly community for all.","title":"Code of Conduct"},{"location":"community/coc/#code-of-conduct","text":"Ibis is governed by the NumFOCUS code of conduct : Quote Be kind to others. Do not insult or put down others. Behave professionally. Remember that harassment and sexist, racist, or exclusionary jokes are not appropriate for IBIS. All communication should be appropriate for a professional audience including people of many different backgrounds. Sexual language and imagery is not appropriate. Ibis is dedicated to providing a harassment-free community for everyone, regardless of gender, sexual orientation, gender identity, and expression, disability, physical appearance, body size, race, or religion. We do not tolerate harassment of community members in any form. Thank you for helping make this a welcoming, friendly community for all.","title":"Code of Conduct"},{"location":"community/ecosystem/","text":"Ecosystem pandas pandas is a Python package that provides fast, flexible, and expressive data structures designed to make working with \"relational\" or \"labeled\" data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real world data analysis in Python. Additionally, it has the broader goal of becoming the most powerful and flexible open source data analysis / manipulation tool available in any language. It is already well on its way towards this goal. SQLAlchemy SQLAlchemy is the Python SQL toolkit and Object Relational Mapper that gives application developers the full power and flexibility of SQL. SQLAlchemy provides a full suite of well known enterprise-level persistence patterns, designed for efficient and high-performing database access, adapted into a simple and Pythonic domain language. sql_to_ibis sql_to_ibis is a Python package that translates SQL syntax into ibis expressions. This allows users to use one unified SQL dialect to target many different backends, even those that don't traditionally support SQL. A good use case would be ease of migration between databases or backends. Suppose you were moving from SQLite to MySQL or from PostgresSQL to BigQuery. These frameworks all have very subtle differences in SQL dialects, but with sql_to_ibis, these differences are automatically translated in Ibis. Another good use case is pandas, which has no SQL support at all for querying a dataframe. With sql_to_ibis this is made possible. For example, import ibis.backends.pandas import pandas import sql_to_ibis df = pandas . DataFrame ({ \"column1\" : [ 1 , 2 , 3 ], \"column2\" : [ \"4\" , \"5\" , \"6\" ]}) ibis_table = ibis . backends . pandas . from_dataframe ( df , name = \"my_table\" , client = ibis . backends . pandas . PandasClient ({}) ) sql_to_ibis . register_temp_table ( ibis_table , \"my_table\" ) sql_to_ibis . query ( \"select column1, cast(column2 as integer) + 1 as my_col2 from my_table\" ) . execute () This would output a dataframe that looks like: column1 my_col2 1 5 2 6 3 7 Ibis on Fugue Fugue is a low-code abstraction layer letting users express the workflows in SQL or Python end-to-end. The design philosophy of Fugue and Ibis is very aligned, and Fugue is at a higher level of abstraction compared to Ibis. So the integration is very intuitive, Ibis is also able to run on all the backends Fugue supports: Pandas, Spark, Dask and DuckDB. The value Fugue adds to Ibis is the seamless integration of SQL semantics and scientific computing plus non-standard SQL operations. The detailed tutorial can be found here Here is an example of a distributed inference pipeline: import pandas as pd import fugue_ibis from fugue import FugueWorkflow # schema: *,pred:double def predict ( df : pd . DataFrame ) -> pd . DataFrame : model = load_model ( \"somefile\" ) return df . assign ( pred = model . predict ( df )) def distributed_predict ( file1 , df2 , dest ): dag = FugueWorkflow () a = dag . load ( file1 ) . as_ibis () b = dag . df ( df2 ) . as_ibis () # ibis operations (you can do more here) joined = a . inner_join ( b , a . key == b . key )[ a , b . f2 ] filtered = joined [ joined . f1 > 0 ] # back to fugue, apply predict distributedly and save filtered . as_fugue () . transform ( predict ) . save ( dest ) return dag # test locally distributed_predict ( small_file , pandas_df2 , temp_dest ) . run () # run on spark when you have a SparkSession: session distributed_predict ( large_file , spark_df2 , dest ) . run ( session )","title":"Ecosystem"},{"location":"community/ecosystem/#ecosystem","text":"","title":"Ecosystem"},{"location":"community/ecosystem/#pandas","text":"pandas is a Python package that provides fast, flexible, and expressive data structures designed to make working with \"relational\" or \"labeled\" data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real world data analysis in Python. Additionally, it has the broader goal of becoming the most powerful and flexible open source data analysis / manipulation tool available in any language. It is already well on its way towards this goal.","title":"pandas"},{"location":"community/ecosystem/#sqlalchemy","text":"SQLAlchemy is the Python SQL toolkit and Object Relational Mapper that gives application developers the full power and flexibility of SQL. SQLAlchemy provides a full suite of well known enterprise-level persistence patterns, designed for efficient and high-performing database access, adapted into a simple and Pythonic domain language.","title":"SQLAlchemy"},{"location":"community/ecosystem/#sql_to_ibis","text":"sql_to_ibis is a Python package that translates SQL syntax into ibis expressions. This allows users to use one unified SQL dialect to target many different backends, even those that don't traditionally support SQL. A good use case would be ease of migration between databases or backends. Suppose you were moving from SQLite to MySQL or from PostgresSQL to BigQuery. These frameworks all have very subtle differences in SQL dialects, but with sql_to_ibis, these differences are automatically translated in Ibis. Another good use case is pandas, which has no SQL support at all for querying a dataframe. With sql_to_ibis this is made possible. For example, import ibis.backends.pandas import pandas import sql_to_ibis df = pandas . DataFrame ({ \"column1\" : [ 1 , 2 , 3 ], \"column2\" : [ \"4\" , \"5\" , \"6\" ]}) ibis_table = ibis . backends . pandas . from_dataframe ( df , name = \"my_table\" , client = ibis . backends . pandas . PandasClient ({}) ) sql_to_ibis . register_temp_table ( ibis_table , \"my_table\" ) sql_to_ibis . query ( \"select column1, cast(column2 as integer) + 1 as my_col2 from my_table\" ) . execute () This would output a dataframe that looks like: column1 my_col2 1 5 2 6 3 7","title":"sql_to_ibis"},{"location":"community/ecosystem/#ibis-on-fugue","text":"Fugue is a low-code abstraction layer letting users express the workflows in SQL or Python end-to-end. The design philosophy of Fugue and Ibis is very aligned, and Fugue is at a higher level of abstraction compared to Ibis. So the integration is very intuitive, Ibis is also able to run on all the backends Fugue supports: Pandas, Spark, Dask and DuckDB. The value Fugue adds to Ibis is the seamless integration of SQL semantics and scientific computing plus non-standard SQL operations. The detailed tutorial can be found here Here is an example of a distributed inference pipeline: import pandas as pd import fugue_ibis from fugue import FugueWorkflow # schema: *,pred:double def predict ( df : pd . DataFrame ) -> pd . DataFrame : model = load_model ( \"somefile\" ) return df . assign ( pred = model . predict ( df )) def distributed_predict ( file1 , df2 , dest ): dag = FugueWorkflow () a = dag . load ( file1 ) . as_ibis () b = dag . df ( df2 ) . as_ibis () # ibis operations (you can do more here) joined = a . inner_join ( b , a . key == b . key )[ a , b . f2 ] filtered = joined [ joined . f1 > 0 ] # back to fugue, apply predict distributedly and save filtered . as_fugue () . transform ( predict ) . save ( dest ) return dag # test locally distributed_predict ( small_file , pandas_df2 , temp_dest ) . run () # run on spark when you have a SparkSession: session distributed_predict ( large_file , spark_df2 , dest ) . run ( session )","title":"Ibis on Fugue"},{"location":"contribute/backend_test_suite/","text":"Working with the Backend Test Suite Before you start This section assumes you've already set up a development environment . You may be able to skip this section If you haven't made changes to any core parts of ibis (e.g., ibis/expr ) or any specific backends ( ibis/backends ) this material isn't necessary to follow to make a pull request. One the primary challenges when working with the ibis codebase is testing. Ibis supports a execution against a number of backends with wildly varying deployment models: Single-node systems like SQLite Multi-node client-server systems like PostgreSQL and MySQL Systems that span a variety of different models like ClickHouse Systems that are designed to run on premises, like Impala This presents quite a challenge for testing. This page is all about how to work with the backend test suite. Systems that can be tested without any additional infrastructure Many backends can be tested without docker-compose . The sqlite , datafusion , and any pandas -based backends can all be tested without needing to do anything except loading data. Backend Testing with Compose Here is the list of backends that can be tested using docker-compose . Backend Docker Compose Services ClickHouse clickhouse PostgreSQL postgres impala impala kudu kudu , impala mysql mysql Testing a Compose Service Check your current directory Make sure you're inside of your clone of the ibis GitHub repository Let's fire up a PostgreSQL server and run tests against it. Start the Service Open a new shell and run docker-compose up --build postgres Test the connection in a different shell using export PGPASSWORD = postgres psql -t -A -h localhost -U postgres -d ibis_testing -c \"select 'success'\" You should see this output: success PostgreSQL doesn't start up instantly It takes a few seconds for postgres to start, so if the previous command fails wait a few seconds and try again Congrats, you now have a PostgreSQL server running and are ready to run tests! First we need to load some data: Load Data Download the data python ci/datamgr.py download In the original terminal, run python ci/datamgr.py postgres You should see a bit of logging, and the command should complete shortly thereafter. Run the test suite You're now ready to run the test suite for the postgres backend: export PYTEST_BACKENDS = postgres pytest ibis/backends/postgres/tests ibis/backends/tests Running the tests may take some time, but eventually they should finish successfully. Please report any failures upstream, even if you're not sure if the failure is bug.","title":"Backend Test Suite"},{"location":"contribute/backend_test_suite/#working-with-the-backend-test-suite","text":"Before you start This section assumes you've already set up a development environment . You may be able to skip this section If you haven't made changes to any core parts of ibis (e.g., ibis/expr ) or any specific backends ( ibis/backends ) this material isn't necessary to follow to make a pull request. One the primary challenges when working with the ibis codebase is testing. Ibis supports a execution against a number of backends with wildly varying deployment models: Single-node systems like SQLite Multi-node client-server systems like PostgreSQL and MySQL Systems that span a variety of different models like ClickHouse Systems that are designed to run on premises, like Impala This presents quite a challenge for testing. This page is all about how to work with the backend test suite.","title":"Working with the Backend Test Suite"},{"location":"contribute/backend_test_suite/#systems-that-can-be-tested-without-any-additional-infrastructure","text":"Many backends can be tested without docker-compose . The sqlite , datafusion , and any pandas -based backends can all be tested without needing to do anything except loading data.","title":"Systems that can be tested without any additional infrastructure"},{"location":"contribute/backend_test_suite/#backend-testing-with-compose","text":"Here is the list of backends that can be tested using docker-compose . Backend Docker Compose Services ClickHouse clickhouse PostgreSQL postgres impala impala kudu kudu , impala mysql mysql","title":"Backend Testing with Compose"},{"location":"contribute/backend_test_suite/#testing-a-compose-service","text":"Check your current directory Make sure you're inside of your clone of the ibis GitHub repository Let's fire up a PostgreSQL server and run tests against it.","title":"Testing a Compose Service"},{"location":"contribute/backend_test_suite/#start-the-service","text":"Open a new shell and run docker-compose up --build postgres Test the connection in a different shell using export PGPASSWORD = postgres psql -t -A -h localhost -U postgres -d ibis_testing -c \"select 'success'\" You should see this output: success PostgreSQL doesn't start up instantly It takes a few seconds for postgres to start, so if the previous command fails wait a few seconds and try again Congrats, you now have a PostgreSQL server running and are ready to run tests! First we need to load some data:","title":"Start the Service"},{"location":"contribute/backend_test_suite/#load-data","text":"Download the data python ci/datamgr.py download In the original terminal, run python ci/datamgr.py postgres You should see a bit of logging, and the command should complete shortly thereafter.","title":"Load Data"},{"location":"contribute/backend_test_suite/#run-the-test-suite","text":"You're now ready to run the test suite for the postgres backend: export PYTEST_BACKENDS = postgres pytest ibis/backends/postgres/tests ibis/backends/tests Running the tests may take some time, but eventually they should finish successfully. Please report any failures upstream, even if you're not sure if the failure is bug.","title":"Run the test suite"},{"location":"contribute/environment/","text":"Setting up a Development Environment There are two primary ways to setup a development environment. nix : fewer steps, isolated conda : more steps, not isolated Required Dependencies git Package Management You need at least one package manager At least one of nix or conda is required to contribute to ibis. Python 3.10 is supported on a best-effort basis As of 2022-01-05 there is experimental support for Python 3.10. However, there are a number of problems with dependencies and development tools that ibis uses and we cannot offically support Python 3.10 until those are fixed. Nix (Linux, Python 3.7-3.9) Miniconda (Linux, Mac, Windows, Python 3.7-3.9) Download and install nix Install gh : nix-shell -p gh # or nix-env -iA gh Fork and clone the ibis repository: # you will likely need to auth, gh will guide you through the steps gh repo fork --clone --remote ibis-project/ibis Run nix-shell in the checkout directory: cd ibis # set up the cache to avoid building everything from scratch nix-shell -p cachix --run 'cachix use ibis' # start a nix-shell # # this may take awhile to download artifacts from the cache nix-shell Mamba is supported as well Mamba and Mambaforge can be used in place of conda . Download and install Miniconda Install gh conda install -c conda-forge gh Fork and clone the ibis repository: gh repo fork --clone --remote ibis-project/ibis Create a Conda environment from a lock file in the repo: cd ibis conda create -n ibis-dev -f conda-lock/<platform-64-pyver>.lock Activate the environment conda activate ibis-dev Install your local copy of ibis into the Conda environment. In the root of the project run: pip install -e . If you want to run the backend test suite you'll need to install docker-compose : conda install docker-compose -c conda-forge","title":"Development Environment"},{"location":"contribute/environment/#setting-up-a-development-environment","text":"There are two primary ways to setup a development environment. nix : fewer steps, isolated conda : more steps, not isolated","title":"Setting up a Development Environment"},{"location":"contribute/environment/#required-dependencies","text":"git","title":"Required Dependencies"},{"location":"contribute/environment/#package-management","text":"You need at least one package manager At least one of nix or conda is required to contribute to ibis. Python 3.10 is supported on a best-effort basis As of 2022-01-05 there is experimental support for Python 3.10. However, there are a number of problems with dependencies and development tools that ibis uses and we cannot offically support Python 3.10 until those are fixed. Nix (Linux, Python 3.7-3.9) Miniconda (Linux, Mac, Windows, Python 3.7-3.9) Download and install nix Install gh : nix-shell -p gh # or nix-env -iA gh Fork and clone the ibis repository: # you will likely need to auth, gh will guide you through the steps gh repo fork --clone --remote ibis-project/ibis Run nix-shell in the checkout directory: cd ibis # set up the cache to avoid building everything from scratch nix-shell -p cachix --run 'cachix use ibis' # start a nix-shell # # this may take awhile to download artifacts from the cache nix-shell Mamba is supported as well Mamba and Mambaforge can be used in place of conda . Download and install Miniconda Install gh conda install -c conda-forge gh Fork and clone the ibis repository: gh repo fork --clone --remote ibis-project/ibis Create a Conda environment from a lock file in the repo: cd ibis conda create -n ibis-dev -f conda-lock/<platform-64-pyver>.lock Activate the environment conda activate ibis-dev Install your local copy of ibis into the Conda environment. In the root of the project run: pip install -e . If you want to run the backend test suite you'll need to install docker-compose : conda install docker-compose -c conda-forge","title":"Package Management"},{"location":"contribute/maintainers_guide/","text":"Maintaining the Codebase Maintainers should be performing a minimum number of tasks, deferring to automation as much as possible: Reviewing pull requests Merging pull requests A number of tasks that are typically associated with maintenance are partially or fully automated: Updating library dependencies: this is handled automatically by WhiteSource Renovate Updating github-actions: this is handled automatically by WhiteSource Renovate Updating nix dependencies: this is a job run at a regular cadence to update nix dependencies Updating dependencies Occasionally you may need to manually lock poetry dependencies, which can be done by running poetry update --lock If a dependency was updated, you'll see changes to poetry.lock in the current directory. Automatic dependency updates WhiteSource Renovate will run at some cadence (outside of traditional business hours) and submit PRs that update dependencies. These upgrades use a conservative update strategy, which is currently to increase the upper bound of a dependency's range. The PRs it generates will regenerate a number of other files so that in most cases contributors do not have to remember to generate and commit these files. Manually updating dependencies Danger Do not manually edit setup.py , it is automatically generated from pyproject.toml Edit pyproject.toml as needed. Run poetry update Run # if using nix ./dev/poetry2setup -o setup.py # it not using nix, requires installation of tomli and poetry-core PYTHONHASHSEED = 42 python ./dev/poetry2setup.py -o setup.py from the repository root. Updates of minor and patch versions of dependencies are handled automatically by renovate . Merging PRs PRs can be merged using the gh command line tool or with the GitHub web UI. Release PyPI Releases to PyPI are handled automatically using semantic release . Ibis is released in two places: PyPI , to enable pip install ibis-framework Conda Forge , to enable mamba install ibis-framework conda-forge The conda-forge package is released using the conda-forge feedstock repository After a release to PyPI, the conda-forge bot automatically updates the ibis package.","title":"Maintainer's Guide"},{"location":"contribute/maintainers_guide/#maintaining-the-codebase","text":"Maintainers should be performing a minimum number of tasks, deferring to automation as much as possible: Reviewing pull requests Merging pull requests A number of tasks that are typically associated with maintenance are partially or fully automated: Updating library dependencies: this is handled automatically by WhiteSource Renovate Updating github-actions: this is handled automatically by WhiteSource Renovate Updating nix dependencies: this is a job run at a regular cadence to update nix dependencies","title":"Maintaining the Codebase"},{"location":"contribute/maintainers_guide/#updating-dependencies","text":"Occasionally you may need to manually lock poetry dependencies, which can be done by running poetry update --lock If a dependency was updated, you'll see changes to poetry.lock in the current directory.","title":"Updating dependencies"},{"location":"contribute/maintainers_guide/#automatic-dependency-updates","text":"WhiteSource Renovate will run at some cadence (outside of traditional business hours) and submit PRs that update dependencies. These upgrades use a conservative update strategy, which is currently to increase the upper bound of a dependency's range. The PRs it generates will regenerate a number of other files so that in most cases contributors do not have to remember to generate and commit these files.","title":"Automatic dependency updates"},{"location":"contribute/maintainers_guide/#manually-updating-dependencies","text":"Danger Do not manually edit setup.py , it is automatically generated from pyproject.toml Edit pyproject.toml as needed. Run poetry update Run # if using nix ./dev/poetry2setup -o setup.py # it not using nix, requires installation of tomli and poetry-core PYTHONHASHSEED = 42 python ./dev/poetry2setup.py -o setup.py from the repository root. Updates of minor and patch versions of dependencies are handled automatically by renovate .","title":"Manually updating dependencies"},{"location":"contribute/maintainers_guide/#merging-prs","text":"PRs can be merged using the gh command line tool or with the GitHub web UI.","title":"Merging PRs"},{"location":"contribute/maintainers_guide/#release","text":"","title":"Release"},{"location":"contribute/maintainers_guide/#pypi","text":"Releases to PyPI are handled automatically using semantic release . Ibis is released in two places: PyPI , to enable pip install ibis-framework Conda Forge , to enable mamba install ibis-framework","title":"PyPI"},{"location":"contribute/maintainers_guide/#conda-forge","text":"The conda-forge package is released using the conda-forge feedstock repository After a release to PyPI, the conda-forge bot automatically updates the ibis package.","title":"conda-forge"},{"location":"contribute/style/","text":"Style and Formatting Code Style The following tools are run in both CI and pre-commit checks to ensure codebase hygiene: Tool Purpose black Formatting Python code isort Formatting and sorting import statements flake8 Linting Python code nix-linter Linting nix files nixpkgs-fmt Formatting nix files shellcheck Linting shell scripts shfmt Formatting shell scripts pyupgrade Ensuring the latest available Python syntax is used Tip If you use nix-shell all of these are setup for you and ready to use, you don't need to install any of these tools. We use numpydoc as our standard format for docstrings. Commit philosophy We aim to make our individual commits small and tightly focused on the feature they are implementing or bug being fixed. If you find yourself making functional changes to different areas of the codebase, we prefer you break up your changes into separate Pull Requests. In general, a philosophy of one Github Issue per Pull Request is a good rule of thumb.","title":"Development Style"},{"location":"contribute/style/#style-and-formatting","text":"","title":"Style and Formatting"},{"location":"contribute/style/#code-style","text":"The following tools are run in both CI and pre-commit checks to ensure codebase hygiene: Tool Purpose black Formatting Python code isort Formatting and sorting import statements flake8 Linting Python code nix-linter Linting nix files nixpkgs-fmt Formatting nix files shellcheck Linting shell scripts shfmt Formatting shell scripts pyupgrade Ensuring the latest available Python syntax is used Tip If you use nix-shell all of these are setup for you and ready to use, you don't need to install any of these tools. We use numpydoc as our standard format for docstrings.","title":"Code Style"},{"location":"contribute/style/#commit-philosophy","text":"We aim to make our individual commits small and tightly focused on the feature they are implementing or bug being fixed. If you find yourself making functional changes to different areas of the codebase, we prefer you break up your changes into separate Pull Requests. In general, a philosophy of one Github Issue per Pull Request is a good rule of thumb.","title":"Commit philosophy"},{"location":"contribute/workflow/","text":"Working on the Codebase Find an issue to work on All contributions are welcome! Code, docs, and constructive feedback are all great contributions to the project. If you don't have a particular issue in mind head over to the GitHub issue tracker for Ibis and look for open issues with the label good first issue . Feel free to help with other issues that aren't labeled as such, but they may be more challenging. Once you find an issue you want to work on, write a comment with the text /take on the issue. GitHub will then assign the issue to you. This lets people know you're working on the issue. If you find an issue that has an assignee, comment on the issue and ask whether the assignee is still working on the issue. Make a branch The first thing you want to do is make a branch. Let's call it useful-bugfix : git checkout -b useful-bugfix Make the desired change Let's say you've made a change to ibis/expr/types.py to fix a bug reported in issue #424242 (not actually an issue). Running git status should give output similar to this: On branch useful-bugfix Your branch is up to date with 'origin/useful-bugfix' . Changes not staged for commit: ( use \"git add <file>...\" to update what will be committed ) ( use \"git restore <file>...\" to discard changes in working directory ) modified: ibis/expr/types.py no changes added to commit ( use \"git add\" and/or \"git commit -a\" ) Run the test suite Next, you'll want to run a subset of the test suite. Required Dependencies You need a development environment before running tests Make sure you've set up a development environment before proceeding To run a subset of the ibis tests use the following command: # The trailing comma in {sqlite/,} is intentional! PYTEST_BACKENDS = \"sqlite\" pytest ibis/backends/ { sqlite/, } /tests You can change PYTEST_BACKENDS=\"sqlite\" to include one or more space-separated supported backends that you want to test. Setting PYTEST_BACKENDS unless you want to run the backend test suite Commit your changes Required Dependencies git cz Tip cz is already installed in your environment if you followed the setup instructions Next, you'll want to commit your changes. Ibis's commit message structure follows the semantic-release conventions . Warning It isn't necessary to use cz commit to make commits, but it is necessary to follow the instructions outlined in this table . Stage your changes and run cz commit : git add . cz commit You should see a series of prompts about actions to take next: Select the type of change you're committing. In this case, we're committing a bug fix, so we'll select fix: ? Select the type of change you are committing (Use arrow keys) \u00bb fix: A bug fix. Correlates with PATCH in SemVer feat: A new feature. Correlates with MINOR in SemVer docs: Documentation only changes style: Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc) refactor: A code change that neither fixes a bug nor adds a feature perf: A code change that improves performance test: Adding missing or correcting existing tests build: Changes that affect the build system or external dependencies (example scopes: pip, docker, npm) ci: Changes to our CI configuration files and scripts (example scopes: GitLabCI) Generally you don't need to think too hard about what category to select, but note that: feat will cause a minor version bump fix will cause a patch version bump everything else will not cause a version bump, unless it's a breaking change (continue reading these instructions for more info on that) Next, you're asked what the scope of this change is: ? What is the scope of this change? (class or file name): (press [enter] to skip) This is optional, but if there's a clear component or single file that is modified you should put it. In our case, let's assume the bug fixed a type inference problem, so we'd type in type-inference at this prompt. You'll then be asked to type in a short description of the change which will be the commit message title: ? Write a short and imperative summary of the code changes: (lower case and no period) fix a type inference issue where floats were incorrectly cast to ints Let's say there was a problem with spurious casting of float to integers, so we type in the message above. That number on the left (here (69) ) is the length of description you've typed in. Next you'll be asked for a longer description, which is entirely optional unless the change is a breaking change , or you feel like a bit of prose ? Provide additional contextual information about the code changes: (press [enter] to skip) A bug was triggered by some incorrect code that caused floats to be incorrectly cast to integers. For non breaking changes, this isn't strictly necessary but it can be very helpful when a change is large, obscure, or complex. For this example let's just reiterate most of what the commit title says. Next you're asked about breaking changes: ? Is this a BREAKING CHANGE? Correlates with MAJOR in SemVer (y/N) If you answer y , then you'll get an additional prompt asking you to describe the breaking changes. This description will ultimately make its way into the user-facing release notes. If there aren't any breaking changes, press enter. Let's say this bug fix does not introduce a breaking change. Finally, you're asked whether this change affects any open issues (ignore the bit about breaking changes) and if yes then to reference them: ? Footer. Information about Breaking Changes and reference issues that this commit closes: (press [enter] to skip) fixes #424242 Here we typed fixes #424242 to indicate that we fixed issue #9000. Whew! Seems like a lot, but it's rather quick once you get used to it. After that you should have a commit that looks roughly like this, ready to be automatically rolled into the next release: commit 4049adbd66b0df48e37ca105da0b9139101a1318 (HEAD -> useful-bugfix) Author: Phillip Cloud <417981+cpcloud@users.noreply.github.com> Date: Tue Dec 21 10:30:50 2021 -0500 fix(type-inference): fix a type inference issue where floats were incorrectly cast to ints A bug was triggered by some incorrect code that caused floats to be incorrectly cast to integers. fixes #424242 Push your changes Now that you've got a commit, you're ready to push your changes and make a pull request! gh pr create Follow the prompts, and gh will print a link to your PR upon successfuly submission.","title":"Commit Workflow"},{"location":"contribute/workflow/#working-on-the-codebase","text":"","title":"Working on the Codebase"},{"location":"contribute/workflow/#find-an-issue-to-work-on","text":"All contributions are welcome! Code, docs, and constructive feedback are all great contributions to the project. If you don't have a particular issue in mind head over to the GitHub issue tracker for Ibis and look for open issues with the label good first issue . Feel free to help with other issues that aren't labeled as such, but they may be more challenging. Once you find an issue you want to work on, write a comment with the text /take on the issue. GitHub will then assign the issue to you. This lets people know you're working on the issue. If you find an issue that has an assignee, comment on the issue and ask whether the assignee is still working on the issue.","title":"Find an issue to work on"},{"location":"contribute/workflow/#make-a-branch","text":"The first thing you want to do is make a branch. Let's call it useful-bugfix : git checkout -b useful-bugfix","title":"Make a branch"},{"location":"contribute/workflow/#make-the-desired-change","text":"Let's say you've made a change to ibis/expr/types.py to fix a bug reported in issue #424242 (not actually an issue). Running git status should give output similar to this: On branch useful-bugfix Your branch is up to date with 'origin/useful-bugfix' . Changes not staged for commit: ( use \"git add <file>...\" to update what will be committed ) ( use \"git restore <file>...\" to discard changes in working directory ) modified: ibis/expr/types.py no changes added to commit ( use \"git add\" and/or \"git commit -a\" )","title":"Make the desired change"},{"location":"contribute/workflow/#run-the-test-suite","text":"Next, you'll want to run a subset of the test suite.","title":"Run the test suite"},{"location":"contribute/workflow/#required-dependencies","text":"You need a development environment before running tests Make sure you've set up a development environment before proceeding To run a subset of the ibis tests use the following command: # The trailing comma in {sqlite/,} is intentional! PYTEST_BACKENDS = \"sqlite\" pytest ibis/backends/ { sqlite/, } /tests You can change PYTEST_BACKENDS=\"sqlite\" to include one or more space-separated supported backends that you want to test. Setting PYTEST_BACKENDS unless you want to run the backend test suite","title":"Required Dependencies"},{"location":"contribute/workflow/#commit-your-changes","text":"","title":"Commit your changes"},{"location":"contribute/workflow/#required-dependencies_1","text":"git cz Tip cz is already installed in your environment if you followed the setup instructions Next, you'll want to commit your changes. Ibis's commit message structure follows the semantic-release conventions . Warning It isn't necessary to use cz commit to make commits, but it is necessary to follow the instructions outlined in this table . Stage your changes and run cz commit : git add . cz commit You should see a series of prompts about actions to take next: Select the type of change you're committing. In this case, we're committing a bug fix, so we'll select fix: ? Select the type of change you are committing (Use arrow keys) \u00bb fix: A bug fix. Correlates with PATCH in SemVer feat: A new feature. Correlates with MINOR in SemVer docs: Documentation only changes style: Changes that do not affect the meaning of the code (white-space, formatting, missing semi-colons, etc) refactor: A code change that neither fixes a bug nor adds a feature perf: A code change that improves performance test: Adding missing or correcting existing tests build: Changes that affect the build system or external dependencies (example scopes: pip, docker, npm) ci: Changes to our CI configuration files and scripts (example scopes: GitLabCI) Generally you don't need to think too hard about what category to select, but note that: feat will cause a minor version bump fix will cause a patch version bump everything else will not cause a version bump, unless it's a breaking change (continue reading these instructions for more info on that) Next, you're asked what the scope of this change is: ? What is the scope of this change? (class or file name): (press [enter] to skip) This is optional, but if there's a clear component or single file that is modified you should put it. In our case, let's assume the bug fixed a type inference problem, so we'd type in type-inference at this prompt. You'll then be asked to type in a short description of the change which will be the commit message title: ? Write a short and imperative summary of the code changes: (lower case and no period) fix a type inference issue where floats were incorrectly cast to ints Let's say there was a problem with spurious casting of float to integers, so we type in the message above. That number on the left (here (69) ) is the length of description you've typed in. Next you'll be asked for a longer description, which is entirely optional unless the change is a breaking change , or you feel like a bit of prose ? Provide additional contextual information about the code changes: (press [enter] to skip) A bug was triggered by some incorrect code that caused floats to be incorrectly cast to integers. For non breaking changes, this isn't strictly necessary but it can be very helpful when a change is large, obscure, or complex. For this example let's just reiterate most of what the commit title says. Next you're asked about breaking changes: ? Is this a BREAKING CHANGE? Correlates with MAJOR in SemVer (y/N) If you answer y , then you'll get an additional prompt asking you to describe the breaking changes. This description will ultimately make its way into the user-facing release notes. If there aren't any breaking changes, press enter. Let's say this bug fix does not introduce a breaking change. Finally, you're asked whether this change affects any open issues (ignore the bit about breaking changes) and if yes then to reference them: ? Footer. Information about Breaking Changes and reference issues that this commit closes: (press [enter] to skip) fixes #424242 Here we typed fixes #424242 to indicate that we fixed issue #9000. Whew! Seems like a lot, but it's rather quick once you get used to it. After that you should have a commit that looks roughly like this, ready to be automatically rolled into the next release: commit 4049adbd66b0df48e37ca105da0b9139101a1318 (HEAD -> useful-bugfix) Author: Phillip Cloud <417981+cpcloud@users.noreply.github.com> Date: Tue Dec 21 10:30:50 2021 -0500 fix(type-inference): fix a type inference issue where floats were incorrectly cast to ints A bug was triggered by some incorrect code that caused floats to be incorrectly cast to integers. fixes #424242","title":"Required Dependencies"},{"location":"contribute/workflow/#push-your-changes","text":"Now that you've got a commit, you're ready to push your changes and make a pull request! gh pr create Follow the prompts, and gh will print a link to your PR upon successfuly submission.","title":"Push your changes"}]}